{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import ptls\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(obj, file)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "import pickle\n",
    "\n",
    "def save_pkl(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ptls-experiments/pytorch-lifestream/tutorials/ensemble_learning'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘age_pred’: File exists\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_DIR = 'age_pred'\n",
    "!mkdir {EXPERIMENT_DIR}\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    'raw',\n",
    "    'composite_1',\n",
    "    'composite_2'\n",
    "]\n",
    "BATCH_SIZE = 64\n",
    "data_dir = '../../../scenario_age_pred/notebooks/data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_coles():\n",
    "    \n",
    "    pl_coles_module = ptls.frames.coles.CoLESModule(\n",
    "        validation_metric=ptls.frames.coles.metric.BatchRecallTopK(\n",
    "            K=4,\n",
    "            metric='cosine',\n",
    "        ),\n",
    "        seq_encoder=torch.nn.Sequential(\n",
    "            ptls.nn.TrxEncoder(\n",
    "                norm_embeddings=False,\n",
    "                embeddings_noise=0.003,\n",
    "                use_batch_norm=False,\n",
    "                embeddings={\n",
    "                    'weekday': {'in': 10, 'out': 8},\n",
    "                    'small_group': {'in': 250, 'out': 16},\n",
    "                    'event_time': {'in': 800, 'out': 8},\n",
    "                },\n",
    "                numeric_values={ \n",
    "                    # 'amount_rur': 'identity',\n",
    "                    # 'amount_rur': 'log',\n",
    "                    # 'amount_rur': LogScaler(*get_norm(df_seq_pretrain_train)),\n",
    "                },\n",
    "            ),\n",
    "            ptls.nn.RnnEncoder(\n",
    "                input_size=32,\n",
    "                type='gru',\n",
    "                hidden_size=128,\n",
    "                is_reduce_sequence=True,\n",
    "            ),\n",
    "        ),\n",
    "        head=ptls.nn.Head(input_size=128, use_norm_encoder=True, hidden_layers_sizes=[256, 256]),\n",
    "        optimizer_partial=partial(torch.optim.Adam, lr=0.001, weight_decay=0.0),\n",
    "        lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9025)\n",
    "    )\n",
    "    return pl_coles_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fedcore.api.utils.data import get_compression_input\n",
    "# from fedcore.api.main import FedCore\n",
    "from ptls.frames.coles.losses.contrastive_loss import ContrastiveLoss\n",
    "from ptls.frames.coles.sampling_strategies.hard_negative_pair_selector import HardNegativePairSelector\n",
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.fedcore_compression.fc_utils import fedcore_fit, extract_loss, get_experimental_setup\n",
    "\n",
    "def run_training(data_dir, define_model, fedcore_setup, save_dir, n_cls=2, folds=range(5)):\n",
    "    log_path = os.path.join(save_dir, 'exp.json')\n",
    "\n",
    "    res = []\n",
    "    for fold_i in tqdm(folds, desc='fold'):\n",
    "        exp_res = {'fold': fold_i} \n",
    "\n",
    "        # data loading\n",
    "        df_trx_pretrain = pd.read_pickle(f'{data_dir}/fold_{fold_i}/df_trx_pretrain.pickle')\n",
    "        df_seq_pretrain = pd.read_pickle(f'{data_dir}/fold_{fold_i}/df_seq_pretrain.pickle')\n",
    "\n",
    "        \n",
    "        df_seq_pretrain_train, df_seq_pretrain_valid = train_test_split(\n",
    "            df_seq_pretrain, test_size=0.5, shuffle=True, random_state=42)\n",
    "        \n",
    "        coles_data_module = ptls.frames.PtlsDataModule(\n",
    "        train_data=ptls.frames.coles.ColesDataset(\n",
    "            data=MemoryMapDataset(\n",
    "                df_seq_pretrain_train.to_dict(orient='records') + \n",
    "                df_trx_pretrain.to_dict(orient='records')\n",
    "            ),\n",
    "            splitter=ptls.frames.coles.split_strategy.SampleSlices(\n",
    "                split_count=5,\n",
    "                cnt_min=25,\n",
    "                cnt_max=200,\n",
    "            ),\n",
    "        ),\n",
    "        valid_data=ptls.frames.coles.ColesDataset(\n",
    "            data=MemoryMapDataset(\n",
    "                df_seq_pretrain_valid.to_dict(orient='records')),\n",
    "            splitter=ptls.frames.coles.split_strategy.SampleSlices(\n",
    "                split_count=5,\n",
    "                cnt_min=25,\n",
    "                cnt_max=100,\n",
    "            ),\n",
    "        ),\n",
    "        train_batch_size=BATCH_SIZE,\n",
    "        train_num_workers=12,\n",
    "        valid_batch_size=BATCH_SIZE,\n",
    "        valid_num_workers=12,\n",
    "        )\n",
    "\n",
    "        # model initialization\n",
    "        model = define_model()\n",
    "        \n",
    "        training_time_0 = datetime.now().timestamp()\n",
    "        exp_setup = get_experimental_setup(fedcore_setup)[0]\n",
    "        exp_setup['need_evo_opt'] = False\n",
    "        exp_setup['need_fedot_pretrain'] = False\n",
    "        exp_setup['distributed_compression'] = False\n",
    "        fedcore_compressor = fedcore_fit(model, coles_data_module, \n",
    "                                         exp_setup, \n",
    "                                         extract_loss(model),\n",
    "                                         n_cls=n_cls)\n",
    "        training_time_1 = datetime.now().timestamp()\n",
    "        exp_res['training_time_0'] = training_time_0\n",
    "        exp_res['training_time_1'] = training_time_1\n",
    "\n",
    "        save_json(res, log_path)\n",
    "\n",
    "        del input_data\n",
    "        del model\n",
    "    return fedcore_compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/ptls-experiments/compression_experiments/age_pred/raw'\n",
    "fcomp = run_training(data_dir, \n",
    "                define_coles,\n",
    "                fedcore_setup = 'raw',\n",
    "    save_dir=save_path,\n",
    "    folds=[0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/ptls-experiments/compression_experiments/age_pred/composite_1'\n",
    "fcomp_1 = run_training(data_dir, \n",
    "                define_coles,\n",
    "                fedcore_setup='composite_1',\n",
    "    save_dir=save_path,\n",
    "    folds=[0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/ptls-experiments/compression_experiments/age_pred/composite_2'\n",
    "fcomp_1 = run_training(data_dir, \n",
    "                define_coles,\n",
    "                fedcore_setup='composite_2',\n",
    "    save_dir=save_path,\n",
    "    folds=[0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.inference_module import InferenceModule\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def eval_metric_evolution(models_dir, data_dir, save_path, folds=[0, 1, 2, 3, 4]):\n",
    "    for file in tqdm(os.listdir(models_dir), 'file'):\n",
    "        if not file.endswith('.pth'): continue\n",
    "        path = Path(models_dir, file)\n",
    "        model = torch.load(path)\n",
    "        evaluate_embs(model, file, data_dir, save_path, folds)\n",
    "\n",
    "\n",
    "def evaluate_embs(coles, MODEL_NAME, DATA_DIR, save_path, folds=[0, 1, 2, 3, 4]):\n",
    "    BATCH_SIZE = 256\n",
    "    for fold_i in tqdm(folds, desc='Fold'):\n",
    "        df_gbm_train = pd.read_pickle(f'{DATA_DIR}/fold_{fold_i}/df_gbm_train.pickle')\n",
    "        if 'event_time' in df_gbm_train.columns:\n",
    "            df_gbm_train['trans_date'] = df_gbm_train['event_time']\n",
    "        df_gbm_test = pd.read_pickle(f'{DATA_DIR}/fold_{fold_i}/df_gbm_test.pickle')\n",
    "\n",
    "\n",
    "        if 'event_time' in df_gbm_test.columns:\n",
    "            df_gbm_test['trans_date'] = df_gbm_test['event_time']\n",
    "\n",
    "        print('Inference started')\n",
    "\n",
    "        inference_dl_gbm_train = torch.utils.data.DataLoader(\n",
    "            dataset=ptls.data_load.datasets.memory_dataset.MemoryMapDataset(\n",
    "                df_gbm_train.to_dict(orient='records'),\n",
    "                i_filters=[\n",
    "                    ptls.data_load.iterable_processing.ISeqLenLimit(max_seq_len=2000), \n",
    "                ],\n",
    "            ),\n",
    "            collate_fn=ptls.data_load.utils.collate_feature_dict,\n",
    "            shuffle=False,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=12,\n",
    "        )\n",
    "\n",
    "        inference_dl_gbm_test = torch.utils.data.DataLoader(\n",
    "            dataset=ptls.data_load.datasets.MemoryMapDataset(\n",
    "                df_gbm_test.to_dict(orient='records'),\n",
    "                i_filters=[\n",
    "                    ptls.data_load.iterable_processing.ISeqLenLimit(max_seq_len=2000), \n",
    "                ],\n",
    "            ),\n",
    "            collate_fn=ptls.data_load.utils.collate_feature_dict,\n",
    "            shuffle=False,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=12,\n",
    "        )\n",
    "\n",
    "        inf_model = InferenceModule(\n",
    "            model=coles.seq_encoder, pandas_output=True, model_out_name='emb')\n",
    "\n",
    "        predict_gbm_train = pl.Trainer(enable_progress_bar=False, logger=None)\\\n",
    "        .predict(inf_model, inference_dl_gbm_train)\n",
    "\n",
    "        predict_gbm_test = pl.Trainer(enable_progress_bar=False, logger=None)\\\n",
    "        .predict(inf_model, inference_dl_gbm_test)\n",
    "\n",
    "        predict_gbm_train = pd.concat(predict_gbm_train, axis=0)\n",
    "\n",
    "        predict_gbm_test = pd.concat(predict_gbm_test, axis=0)\n",
    "\n",
    "        predict_gbm_train.set_index('client_id', inplace=True)\n",
    "        predict_gbm_test.set_index('client_id', inplace=True)\n",
    "\n",
    "        gbm_model = LGBMClassifier(**{\n",
    "            'n_estimators': 1000,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': 4,\n",
    "            'metric': 'multi_error',\n",
    "            'learning_rate': 0.02,\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'feature_fraction': 0.75,\n",
    "            'colsample_bytree': None,\n",
    "            'max_depth': 12,\n",
    "            'lambda_l1': 1,\n",
    "            'reg_alpha': None,\n",
    "            'lambda_l2': 1,\n",
    "            'reg_lambda': None,\n",
    "            'min_data_in_leaf': 50,\n",
    "            'min_child_samples': None,\n",
    "            'num_leaves': 50,\n",
    "            'random_state': 42,\n",
    "            'n_jobs': 4,\n",
    "        })\n",
    "        print('GBM started')\n",
    "        pca = PCA(0.95)\n",
    "        gbm_model.fit(pca.fit_transform(predict_gbm_train.drop(columns='bins')), predict_gbm_train['bins'])\n",
    "\n",
    "        acc = accuracy_score(\n",
    "            gbm_model.predict(pca.transform(predict_gbm_test.drop(columns='bins'))), \n",
    "            predict_gbm_test['bins'],\n",
    "        )\n",
    "\n",
    "        with open(save_path, 'at') as f:\n",
    "            print('\\t'.join([\n",
    "                MODEL_NAME,\n",
    "                f'{datetime.now():%Y-%m-%d %H:%M:%S}',\n",
    "                f'{fold_i}',\n",
    "                'accuracy',\n",
    "                f'{acc:.4f}',\n",
    "        ]), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric_evolution(\n",
    "    '/ptls-experiments/compression_experiments/age_pred/raw/checkpoints',\n",
    "    data_dir,\n",
    "    '/ptls-experiments/compression_experiments/age_pred/raw/results.txt',\n",
    "    [0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric_evolution(\n",
    "    '/ptls-experiments/compression_experiments/age_pred/composite_1/checkpoints',\n",
    "    data_dir,\n",
    "    '/ptls-experiments/compression_experiments/age_pred/composite_1/results.txt',\n",
    "    [0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric_evolution(\n",
    "    '/ptls-experiments/compression_experiments/age_pred/composite_2/checkpoints',\n",
    "    data_dir,\n",
    "    '/ptls-experiments/compression_experiments/age_pred/composite_2/results.txt',\n",
    "    [0]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptls-experiments-H-SwwRmK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
