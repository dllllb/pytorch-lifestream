{
  data_module: {
    type: map

    setup: {
      dataset_files: {
        train_data_path: "data/train_trx_file.parquet"
        test_data_path: "data/test_trx_file.parquet"
      }
      col_id: client_id
      col_id_dtype: int
      col_target: bins

      split_by: embeddings_validation
      fold_info: "conf/embeddings_validation.work/folds/folds.json"
    }

    train: {
        min_seq_len: 0
        augmentations: [
          [RandomSlice, {min_len: 150, max_len: 350}]
          [DropoutTrx, {trx_dropout: 0.005}]
        ]
        num_workers: 16
        batch_size: 32
        
    }

    valid: {
      augmentations: [
        [SeqLenLimit, {max_seq_len: 1200}]
      ]
      num_workers: 8
      batch_size: 512
    }
  }

  embedding_validation_results: {
    model_name: nn
    feature_name: target_scores
    output_path: "results/fit_target_results.json"
  }

  seed_everything: 42
  trainer: {
    gpus: 1
    auto_select_gpus: false

    max_epochs: 35

    checkpoint_callback: false
    deterministic: true
  }
  logger_name: target_scores

  params: {
    score_metric: accuracy

    encoder_type: rnn
    rnn: {
      hidden_size: 48
      type: gru
      bidir: false
      trainable_starter: static
    }

    trx_encoder: {
      norm_embeddings: false
      embeddings_noise: 0.003
      embeddings: {
        trans_date: {in: 800, out: 8}
        small_group: {in: 250, out: 16}
      }
      numeric_values: {
        amount_rur: identity
      }
    }

    head_layers: [
      # [BatchNorm1d, {num_features: "{seq_encoder.embedding_size}"}]
      [Linear, {"in_features": "{seq_encoder.embedding_size}", "out_features": 4}]
      [LogSoftmax, {dim: 1}]
    ]

    train: {
      random_neg: false
      loss: NLLLoss
      lr: 0.01
      weight_decay: 0.0
    }

    lr_scheduler: {
      step_size: 5
      step_gamma: 0.5
    }
  }
}
