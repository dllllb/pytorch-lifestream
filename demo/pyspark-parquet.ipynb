{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf847b53",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Pyspark Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/22 13:26:39 WARN Utils: Your hostname, vm2 resolves to a loopback address: 127.0.1.1; using 192.168.0.6 instead (on interface ens192)\n",
      "22/07/22 13:26:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/07/22 13:26:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/22 13:26:40 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/07/22 13:26:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/07/22 13:26:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/07/22 13:26:41 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '192.168.0.6'),\n",
       " ('spark.driver.memoryOverhead', '4g'),\n",
       " ('spark.app.id', 'local-1658496401736'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/kireev/pycharm-deploy/pytorch-lifestream/demo/spark-warehouse'),\n",
       " ('spark.local.dir', '../../spark_local_dir'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.app.startTime', '1658496400131'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '46639'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PysparkDataPreprocessor'),\n",
       " ('spark.cores.max', '24'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.executor.memoryOverhead', '4g')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data_path = 'data/'\n",
    "\n",
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"PysparkDataPreprocessor\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"24\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27115f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "data, _ = urlretrieve(\n",
    "    'https://huggingface.co/datasets/dllllb/age-group-prediction/resolve/main/transactions_train.csv.gz?download=true',\n",
    "    'transactions_train.csv.gz')\n",
    "\n",
    "source_data = spark.read.options(header=True, inferSchema=True).csv(data)\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c7afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    col_event_time='trans_date',\n",
    "    event_time_transformation='none',\n",
    "    cols_category=['small_group'],\n",
    "    cols_numerical=['amount_rur'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d43a15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/22 13:27:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/22 13:27:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/22 13:27:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/07/22 13:27:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 10:===================================================>  (192 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 31 ms, total: 164 ms\n",
      "Wall time: 23.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_pysparkdf = preprocessor.fit_transform(source_data).persist()\n",
    "dataset_pysparkdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7582b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|client_id|          trans_date|         small_group|          amount_rur|          event_time|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      463|[1, 2, 5, 7, 8, 1...|[7, 1, 7, 3, 3, 2...|[62.14, 3.617, 43...|[1, 2, 5, 7, 8, 1...|\n",
      "|      471|[0, 1, 1, 1, 1, 2...|[68, 1, 7, 4, 5, ...|[29.217, 135.852,...|[0, 1, 1, 1, 1, 2...|\n",
      "|      496|[3, 3, 4, 5, 6, 8...|[1, 3, 1, 1, 1, 4...|[59.25, 32.057, 2...|[3, 3, 4, 5, 6, 8...|\n",
      "|      833|[1, 1, 1, 2, 2, 3...|[1, 15, 17, 36, 4...|[45.413, 7.117, 4...|[1, 1, 1, 2, 2, 3...|\n",
      "|     1238|[0, 3, 9, 10, 10,...|[3, 11, 3, 1, 3, ...|[4.579, 56.583, 8...|[0, 3, 9, 10, 10,...|\n",
      "|     1342|[0, 4, 11, 11, 13...|[14, 3, 21, 3, 3,...|[3.435, 11.448, 7...|[0, 4, 11, 11, 13...|\n",
      "|     1591|[2, 2, 3, 3, 17, ...|[20, 37, 6, 7, 1,...|[43.084, 91.544, ...|[2, 2, 3, 3, 17, ...|\n",
      "|     1645|[2, 3, 3, 4, 4, 5...|[2, 1, 16, 2, 1, ...|[20.141, 52.482, ...|[2, 3, 3, 4, 4, 5...|\n",
      "|     1829|[7, 9, 10, 11, 12...|[3, 1, 3, 1, 1, 2...|[18.319, 35.106, ...|[7, 9, 10, 11, 12...|\n",
      "|     1959|[3, 3, 4, 5, 12, ...|[1, 3, 1, 1, 30, ...|[44.939, 43.127, ...|[3, 3, 4, 5, 12, ...|\n",
      "|     2122|[0, 4, 5, 5, 6, 7...|[38, 5, 6, 14, 3,...|[0.045, 7.154, 6....|[0, 4, 5, 5, 6, 7...|\n",
      "|     2142|[2, 2, 13, 18, 20...|[10, 7, 1, 24, 11...|[836.497, 41.135,...|[2, 2, 13, 18, 20...|\n",
      "|     2659|[27, 31, 39, 48, ...|[3, 3, 36, 3, 1, ...|[27.477, 21.564, ...|[27, 31, 39, 48, ...|\n",
      "|     3175|[6, 6, 7, 10, 10,...|[7, 4, 1, 2, 1, 2...|[81.106, 25.05, 2...|[6, 6, 7, 10, 10,...|\n",
      "|     3749|[1, 4, 4, 4, 4, 4...|[3, 10, 1, 46, 35...|[20.608, 28.256, ...|[1, 4, 4, 4, 4, 4...|\n",
      "|     3794|[3, 3, 4, 7, 8, 9...|[10, 5, 26, 26, 7...|[90.381, 23.677, ...|[3, 3, 4, 7, 8, 9...|\n",
      "|     3997|[0, 1, 2, 2, 3, 5...|[3, 1, 1, 29, 1, ...|[99.193, 156.711,...|[0, 1, 2, 2, 3, 5...|\n",
      "|     4101|[3, 4, 5, 6, 6, 7...|[1, 1, 1, 1, 13, ...|[16.288, 15.083, ...|[3, 4, 5, 6, 6, 7...|\n",
      "|     4818|[1, 2, 4, 6, 6, 6...|[1, 6, 6, 1, 6, 2...|[29.492, 23.722, ...|[1, 2, 4, 6, 6, 6...|\n",
      "|     4935|[3, 4, 4, 4, 8, 8...|[31, 10, 3, 31, 3...|[460.609, 93.605,...|[3, 4, 4, 4, 8, 8...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_pysparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35e3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client_id', 'int'),\n",
       " ('trans_date', 'array<int>'),\n",
       " ('small_group', 'array<int>'),\n",
       " ('amount_rur', 'array<double>'),\n",
       " ('event_time', 'array<int>')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pysparkdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3510707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dataset: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset 21699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of valid dataset 2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = dataset_pysparkdf.sample(fraction=0.2)\n",
    "train_df = dataset_pysparkdf.subtract(test_df)\n",
    "\n",
    "valid_df = train_df.sample(fraction=0.1)\n",
    "train_df = train_df.subtract(valid_df)\n",
    "\n",
    "print('Size of test dataset:', test_df.count())\n",
    "print('Size of train dataset', train_df.count())\n",
    "print('Size of valid dataset', valid_df.count())\n",
    "\n",
    "test_df.write.parquet('test.parquet', mode='overwrite')\n",
    "train_df.write.parquet('train.parquet', mode='overwrite')\n",
    "valid_df.write.parquet('valid.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Data access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781deed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import ParquetDataset, ParquetFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1c0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable_train = ParquetDataset(ParquetFiles('train.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e99e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 20355,\n",
       " 'trans_date': tensor([  0,   1,   2,  ..., 727, 727, 727], dtype=torch.int32),\n",
       " 'small_group': tensor([ 4,  3,  6,  ...,  2,  3, 18], dtype=torch.int32),\n",
       " 'amount_rur': tensor([16.3020,  6.6860,  6.6400,  ...,  0.0430, 10.7820,  3.6660],\n",
       "        dtype=torch.float64),\n",
       " 'event_time': tensor([  0,   1,   2,  ..., 727, 727, 727], dtype=torch.int32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(iterable_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354c3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7cb807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\n",
    "\n",
    "map_processed_train = MemoryMapDataset(\n",
    "    data=iterable_train,\n",
    "    i_filters=[\n",
    "        SeqLenFilter(min_seq_len=25),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8086734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 20355,\n",
       " 'trans_date': tensor([  0,   1,   2,  ..., 727, 727, 727], dtype=torch.int32),\n",
       " 'small_group': tensor([ 4,  3,  6,  ...,  2,  3, 18], dtype=torch.int32),\n",
       " 'amount_rur': tensor([16.3020,  6.6860,  6.6400,  ...,  0.0430, 10.7820,  3.6660],\n",
       "        dtype=torch.float64),\n",
       " 'event_time': tensor([  0,   1,   2,  ..., 727, 727, 727], dtype=torch.int32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_processed_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ab1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da2f289b",
   "metadata": {},
   "source": [
    "**Attention!**\n",
    "\n",
    "You cannot use a pretrained `coles-emb.pt` with a different preprocessor than the one used to train the model.\n",
    "This is because preprocessor save specific category to embedding_id mapping during fit procedure.\n",
    "Model use this mapping during pretrain.\n",
    "Using different mapping at inference will corrupt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptls",
   "language": "python",
   "name": "ptls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
