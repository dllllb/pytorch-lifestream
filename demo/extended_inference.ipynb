{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bc575c",
   "metadata": {},
   "source": [
    "# Extended inference\n",
    "\n",
    "This demo shows the following inference options:\n",
    "\n",
    "- big data inference\n",
    "- iterable data load and inference\n",
    "- labels passing\n",
    "\n",
    "We will generage wyth `pyspark` a big dataset with transactions and other fields.\n",
    "\n",
    "Since the dataset is large, we will use an iterable dataset to save memory and enable data loading and GPU output in parallel.\n",
    "\n",
    "We don't need only embeddings. We need them with user_id, target, labels and other fields. We should keep it together for large iterable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ef79f",
   "metadata": {},
   "source": [
    "## Data load\n",
    "\n",
    "We will use real transactions to generate our fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ab2a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239M  100  239M    0     0  11.1M      0  0:00:21  0:00:21 --:--:-- 11.3M\n",
      "Archive:  age-prediction-nti-sbebank-2019.zip\n",
      "  inflating: data/test.csv           \n",
      "  inflating: data/small_group_description.csv  \n",
      "  inflating: data/train_target.csv   \n",
      "  inflating: data/transactions_train.csv  \n",
      "  inflating: data/transactions_test.csv  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data/transactions_train.csv'):\n",
    "    ! mkdir -p data\n",
    "    ! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "    ! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d data\n",
    "    ! mv age-prediction-nti-sbebank-2019.zip data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d465e5",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6974773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50384ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 15:55:28 WARN Utils: Your hostname, vm2 resolves to a loopback address: 127.0.1.1; using 192.168.0.6 instead (on interface ens192)\n",
      "22/06/29 15:55:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 15:55:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/29 15:55:29 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "spark_conf = SparkConf()\n",
    "spark_conf.setMaster(\"local[8]\").setAppName(\"pyspark_data_generation\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"32g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"32g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.local.dir\", \"./spark_local_dir\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f214099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_data_generation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc7f8bb8850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ad691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec99680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04451d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b096417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d08382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.462997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.016998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>15.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>21.341000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date small_group  amount_rur\n",
       "0          6           4   71.462997\n",
       "1          6          35   45.016998\n",
       "2          8          11   13.887000\n",
       "3          9          11   15.983000\n",
       "4         10          11   21.341000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_real_trx = spark.read.csv('data/transactions_train.csv', header=True) \\\n",
    "    .select('trans_date', 'small_group', F.col('amount_rur').cast('float'))\n",
    "df_real_trx.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d873d75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26450577"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_trx_count = df_real_trx.count()\n",
    "real_trx_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848095a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_CNT = 1e6\n",
    "TOTAL_TRX_FOR_GENERATE = 800 * USER_CNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f28aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated = df_real_trx.sample(fraction=TOTAL_TRX_FOR_GENERATE / real_trx_count, withReplacement=True)\n",
    "df_generated = df_generated.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50cb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated = df_generated.withColumn('user_id', F.concat(F.lit('u_'), F.floor(F.rand() * USER_CNT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e464dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated = df_generated.withColumn('event_time', F.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c58c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>151.056000</td>\n",
       "      <td>u_747178</td>\n",
       "      <td>0.954769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401</td>\n",
       "      <td>34</td>\n",
       "      <td>5.866000</td>\n",
       "      <td>u_695101</td>\n",
       "      <td>0.929234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504</td>\n",
       "      <td>47</td>\n",
       "      <td>23.871000</td>\n",
       "      <td>u_434036</td>\n",
       "      <td>0.973693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>673</td>\n",
       "      <td>1</td>\n",
       "      <td>22.753000</td>\n",
       "      <td>u_994207</td>\n",
       "      <td>0.964114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "      <td>146.634003</td>\n",
       "      <td>u_500597</td>\n",
       "      <td>0.379053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date small_group  amount_rur   user_id  event_time\n",
       "0         41          21  151.056000  u_747178    0.954769\n",
       "1        401          34    5.866000  u_695101    0.929234\n",
       "2        504          47   23.871000  u_434036    0.973693\n",
       "3        673           1   22.753000  u_994207    0.964114\n",
       "4         87          83  146.634003  u_500597    0.379053"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generated.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efdeff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 52.8 ms, total: 183 ms\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_generated.write.parquet('df_generated.parquet', mode='overwrite')\n",
    "df_generated = spark.read.parquet('df_generated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934eaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a1f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 583 ms, sys: 929 ms, total: 1.51 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_generated.groupby('user_id').count().select('count') \\\n",
    ".sample(True, fraction=1e4 / USER_CNT).toPandas().hist(bins=50, figsize=(12, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e59529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    cols_event_time='event_time',\n",
    "    time_transformation='none',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[],\n",
    "    cols_identity=[\"amount_rur\"],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ddba40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 16:03:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:====================================================> (196 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 16:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/06/29 16:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "CPU times: user 201 ms, sys: 128 ms, total: 329 ms\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_gen_features = preprocessor.fit_transform(df_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d7595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen_features = df_gen_features.withColumn('target', F.round(F.rand()).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b7d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:====================================>                 (136 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 16:12:14 WARN MemoryStore: Not enough space to cache rdd_79_136 in memory! (computed 68.2 MiB so far)\n",
      "22/06/29 16:12:14 WARN BlockManager: Persisting block rdd_79_136 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 376 ms, sys: 62.5 ms, total: 439 ms\n",
      "Wall time: 11min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_gen_features.write.parquet('df_gen_features.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8eb8287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5G\tdf_gen_features.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh df_gen_features.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8668b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "843e1c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trans_date': 730, 'small_group': 202}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.get_category_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab7d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d704f4b7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fd372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2052c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f248df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b254e6c3",
   "metadata": {},
   "source": [
    "## Data for inferenfce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893d9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from ptls.data_load.datasets import ParquetDataset, ParquetFiles\n",
    "from ptls.data_load.iterable_processing import ISeqLenLimit\n",
    "from ptls.data_load.utils import collate_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe1dec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'u_100173',\n",
       " 'trans_date': tensor([471, 430, 185, 616, 416, 609, 149, 595, 718, 284, 391, 158, 581, 699,\n",
       "         379, 457, 504, 704, 694, 306, 469, 294, 358, 104, 168, 726, 334, 598,\n",
       "         399, 397, 226,  88, 548, 413, 153, 490, 464, 144, 454, 171, 492, 390,\n",
       "         257, 387,  70, 502, 232, 107, 403,  38, 280, 159, 623, 472, 523, 261,\n",
       "         343, 393,  79, 281,  60,  82, 554, 276, 284, 161,  35, 221,  51,  44,\n",
       "          89, 497, 280, 141, 407,  20, 651,  56, 300, 309, 458,  63, 100, 325,\n",
       "         177, 131, 411, 453,  41, 392, 473, 517, 728, 312, 223, 163, 593, 348,\n",
       "         538, 231, 228, 615, 188, 162, 529, 159, 291, 182,  86,  76, 614, 115,\n",
       "         372, 175, 555, 224,   8, 479, 286, 144, 139, 127, 330, 682, 217, 379,\n",
       "         616, 486, 220, 399, 152, 218, 111, 675, 124, 662, 254, 345, 618, 466,\n",
       "         104, 559, 699, 235, 186,  58, 287, 497, 149, 123, 704,  12, 582,  64,\n",
       "         331, 629,  14, 410,  55, 602, 101, 539,  65, 617, 410,  92,  15, 386,\n",
       "          77, 213, 370, 439, 593, 243, 118, 513, 189, 185, 584, 691, 428, 506,\n",
       "         421, 286, 134, 108, 535, 240, 307, 370, 442,  51, 143, 252, 113, 639,\n",
       "         260, 608, 200, 423, 206, 201,  62, 420,  69, 438, 211, 272,  90, 702,\n",
       "           5, 278, 314,  33, 238, 398, 308, 401,  94, 118, 586,  40,  71, 554,\n",
       "         247, 194, 635, 650,  36, 564, 485,  55, 165, 459, 258, 191, 196, 112,\n",
       "         119, 273, 205, 549,  60,   4, 190, 337, 113,  10,  52, 512, 185,  46,\n",
       "         122, 218, 222,  16, 576, 226, 606, 602,  54, 490, 294, 372, 376, 343,\n",
       "         534,  88,  47, 418,  24, 424, 121,  66, 419, 483, 348, 341, 243, 171,\n",
       "         402,  79, 303, 367, 387, 556, 409,   8, 252, 467, 164, 413, 164,  25,\n",
       "         152, 135, 127,  63, 606, 575, 312, 186, 414, 690, 322, 654,  29, 171,\n",
       "         206,   3, 410,  34, 478, 186, 304, 583, 262,  49, 703, 147, 668, 249,\n",
       "         308, 560,  99,  10, 604, 160, 334, 174, 323, 541, 707, 197, 470, 355,\n",
       "         602, 578, 104, 569, 410, 563, 690, 107, 400, 166, 189, 506, 553, 602,\n",
       "          26, 196,  96, 193, 163, 705,  10, 410, 303, 488, 373, 515, 463, 224,\n",
       "         124, 402, 649, 134,  11, 577, 515, 126, 603, 702, 689, 414, 249, 567,\n",
       "          40,  36, 433, 414, 322, 469, 617, 139, 620, 293, 223, 203,  42, 206,\n",
       "           2, 194, 582, 190, 133, 385, 674, 680, 576, 439, 674, 683, 234, 626,\n",
       "          46, 377, 724,  69, 380, 435, 367, 154, 392, 261, 591, 178, 263, 389,\n",
       "         328, 141, 504, 465,  42, 346, 130, 314, 543, 157, 292, 631,  93, 676,\n",
       "         622, 273,  30, 198, 259, 418, 174,  89, 490,  71, 212, 595, 528, 571,\n",
       "         184,  79,  60, 445, 172, 213, 276, 144, 682, 719, 610, 381, 644, 315,\n",
       "           5, 214, 586, 247, 460,  61, 242, 128, 205, 705, 387, 619, 522, 235,\n",
       "          39, 560, 589, 293, 434, 664, 324,  33, 466, 286, 387, 416, 303,  73,\n",
       "         326, 170, 463, 340,  49, 644, 126,  61, 338, 529, 293, 208, 125, 445,\n",
       "         258, 253, 393,  98, 277, 382,  84, 474, 458,  84, 452, 441,  22, 427,\n",
       "         131, 409, 274, 453, 619,  99, 201, 492,  56, 583, 584, 623, 296, 467,\n",
       "         286, 420,  33, 154, 133, 313, 712, 316, 215,  70, 255, 699, 647,  22,\n",
       "         178, 640, 170, 346, 352, 326, 355, 444, 348,  26,  37, 488, 228, 451,\n",
       "         403,  87,   5, 203, 701,  69, 324, 276, 205, 115, 181,  54, 218, 108,\n",
       "         275, 463,  46, 335, 159, 405, 461, 535, 103, 693, 456, 145, 541, 526,\n",
       "         576, 283, 167, 663, 258, 188, 175, 335, 221, 328, 482, 622,  67, 695,\n",
       "         542, 183, 226, 268, 543,  44, 365, 271, 596, 162, 374,  28, 542, 341,\n",
       "         698, 534, 417,  11, 156, 267, 256, 230, 221, 614, 236, 478, 201, 716,\n",
       "          95,  30,  52, 533, 671, 479, 473, 692, 458,  73, 460, 182, 247, 251,\n",
       "         528, 107, 229, 121, 502, 278, 359,  75, 282, 117,  32, 403, 511, 522,\n",
       "          33, 454,  42, 362, 430,  31, 414, 392, 264, 106, 648, 534, 244, 352,\n",
       "         220, 570, 178, 508, 705,  14, 169, 227, 412, 323, 251, 146, 348, 567,\n",
       "         354, 591, 258, 243, 511, 221,  77, 716, 626, 426, 306,  32,  99,  22,\n",
       "         115,  48, 313,  42, 224, 201, 354,  73, 662, 378, 116,   6,  28, 272,\n",
       "         200,  62, 395, 238, 596, 426, 692, 424,   5, 471, 122, 246, 321,   4,\n",
       "         155, 212, 106, 717,  82, 315, 330, 231, 154, 130, 365, 215, 474, 533,\n",
       "         611, 410, 621, 648, 701, 405, 121, 386, 483, 429, 162, 125, 586, 405,\n",
       "         268, 296, 491, 387,  54, 598, 205,  39, 318, 309, 327,  18, 289, 556,\n",
       "         130, 577, 286, 124, 182, 294, 136, 399, 461, 150, 349, 428, 676, 155,\n",
       "         328,  56, 612, 394, 306, 681, 468, 527, 533, 602, 349, 604, 273,  44,\n",
       "         146, 422,  95,  33,  22, 566, 360, 594, 307, 517, 470, 120, 373, 455,\n",
       "         502, 258, 295, 391, 356, 628,  18, 641,  61, 434,  22, 511, 615, 269,\n",
       "          27, 628, 431, 212], dtype=torch.int32),\n",
       " 'small_group': tensor([  2,   6,  10,   1,   3,   3,   4,   1,  15,   1,   2,  14,  17,  17,\n",
       "           1,   4,   4,   5,  14,   1,   1,   3,  10,   1,   1,  19,   6,  55,\n",
       "           1,   5,   1,   6,  11,  35,   1,   1,   5,   2,   3,   6,  68,   8,\n",
       "           1,   3,   9,   5,   1,   1,   2,   3,   1,   1,   4,   1,   2,   1,\n",
       "           3,   1,   4,   1,   8,   1,   1,   1,   1,  11,   7,  28,   2,   8,\n",
       "           1,   1,   5,   9,   1,   5,   3,   3,   1,  14,   4,   1,  10,   2,\n",
       "           1,   7,   1,   1,   1,   1,   1,  13,   3,  20,   3,  12,  34,   1,\n",
       "          19,  11,   1,   4,   1,   1,   1,  18,  12,   3,   1,   8,  11,   1,\n",
       "           2,   2,   1,   6,  58,   6,   1,   2,  61,   4,  47,   3,   2,   3,\n",
       "          28,   1,  39,   1,   3,   2,   2,   3,  11,   6,   2,   7,   3,   5,\n",
       "           6,   2,   6,   1,   4,   9,   8,  49,   2,   1,   3,   9,   1,   3,\n",
       "          77,  25,  12,   1,  25,   6,   3,   1,  13,  17,   7,   1,  37,   1,\n",
       "           5,   1,   2,   1,   3,   9,   1,   6,   4,   1,  11,   7,   4,  11,\n",
       "           9,   7,  48,  13,   1,   5,  26,  19,   6,   2,   6,   1,   4,  87,\n",
       "          15,   3,   3,  54,  12,   2,   1,   2,  13,   1,   1,  32,   2,   3,\n",
       "           1,   5,  10,   6,   1,  13,  23,  66,   6,   1,   3,   4,  23,  12,\n",
       "           2,   1,   3,  89,   2,   2,  12,   3,   1,   3,   1,  12,   1,   2,\n",
       "           4,   1,   1,   3,  24,   8,   8,   3,   3,  10,   8,   1,   2,   1,\n",
       "           2,   2,   2,   1,   1,   7,   8,   1,   3,   1,   1,   1,   1,   1,\n",
       "           1,   1,   9,  10,  12,   1,  17,  21,   2,   1,   1,   3,   1,   7,\n",
       "           6,   8,   6,   1,   1,   1,  10,   4,   4,   9,   5,  10,  19,   3,\n",
       "           7,   3,   1,   3,   2,   7,  23,   1,   2,  51,   8,   6,   5,   4,\n",
       "           1,   1,   4,   9,  39,   7,   1,   1,  33,   2,   4,   3,  21,  85,\n",
       "          12,   7, 113,   2,   5,   5,   1,   2,   8,   1,   8,   1,   1,   2,\n",
       "           7,   1,  37,  62,  13,   3,   5,   3,  52,   2,   5,   3,   2,   1,\n",
       "           1,  12,   1,   6,   5,   5,   2,   1,   2,  11,   3,   8,  14,   1,\n",
       "           3,  10,   4,  16,  16,   1,   1,   2,   4,  35,  41,  12,   1,   2,\n",
       "           1,  17,   4,   9,   5,   1,   3,   2,  15,   1,  12,   3,   1,   4,\n",
       "           3,  15,  29,  10,   2,   3,   2,   2,   1,   4,  23,   6,   1,  17,\n",
       "           1,  22,   1,  41,   1,   1,   4,  66,   5,   1,   5,   4,   1,   2,\n",
       "          19,   1,  10,   1,   5,   5,   9,   1,   1,  94,  31,   1,  20,   1,\n",
       "           1,   8,  15,   3,   3,   2,   4,   1,   1,   4,   1,   1,   6,   1,\n",
       "           2,  38,   1,   8,   8,   8,   1,   1,  52,   2,   1,  10,   2,   4,\n",
       "           9,   4,   1,   8,   6,   6,   1,   4,   1,   2,  62,   7,   9,   1,\n",
       "           1,   2,   8,  36,   1,   1,   1,   4,   4,   2,   9,   2,   2,   6,\n",
       "           1,   1,   1,   2, 108,   7,   1,   1,   7,  31,   1,   3,   2,  14,\n",
       "          25,  33,  12,   4,   6,   2,   1,   1,   2,   1,   6,   1,   2,   1,\n",
       "           1,   1,   4,   1,   1,   7,  53,   1,   1,  10,   3,   1,   4,   6,\n",
       "           5,  33,  21,  13,   9,   2,   1,  58,  14,   1,   2,   5,   1,   1,\n",
       "           1,   7,   3,   1,  69,  10,   1,  10,   6,   1,   5,  14,  16,   3,\n",
       "           1,   1,  13,   1,  12,   5,  75,   1,   1,   1,  10,   3,   1,   1,\n",
       "           5,   1,   4,   1,  14,   1,   1,   4,   1,   2,   1,   2,   4,  68,\n",
       "           2,  13,  10,   1,   1,   2,   4,  23,  54,   3,   1,   2,   2,   3,\n",
       "          12,   2,   7,   1,  15,   3,   1,   8,   1,  56,   4,  38,   7,   2,\n",
       "           5,  38,   1,   1,   3,   9,   1,   1,  69,  42,   2,   2,   1,   1,\n",
       "           1,   4,   1,   1,   2,   4,  19,   4,   1,   2,   3,   3,   1,   2,\n",
       "           1,   9,  24,   1,  33,   9,   1,   1,   3,   2,   1,  54,   1,  51,\n",
       "           9,   1,   1,   3,   6,  68,  25,   2,  12,   1,   1,   3,   3,   7,\n",
       "           3,   1,  11,   4,   1,  25,  15,   2,   8,   1,   1,  17,   4,   6,\n",
       "           4,   3,   5,   6,   9,  12,   1,   1,   1,  26,   2,   2,   8,   1,\n",
       "          11,   1,   6,   8,   2,  10,   3,  10,   1,   1,   1,   1,   1,   4,\n",
       "           1,   2,   1,   5,   4,  62,   8,  10,   2,   1,   7,   9,   5,   6,\n",
       "           2,   1,  10,   3,   1,   1,  10,   3,   1,   1,  10,   5,   1,   1,\n",
       "          10,   1,   4,   4,  32,   4,   5,   1,   1,   1,   1,   5,   2,   1,\n",
       "           1,  73,   1,  52,  13,   1,   1,   4,   2,  23,   8,   1,   5,   1,\n",
       "           2,  21,  12,   3,   2,   8,   3,   4,   5,   2,   5,  98,   5,  52,\n",
       "           2,   4,  26,   1,   9,   2,  13,   3,   5,   7,   6,   2,   8,   1,\n",
       "          32,   1,   5,  26,   4,   5,   2,   3,  17,   5,   6,   3,   3,  12,\n",
       "           7,   3,   5,   1,   1,   3,   4,   1,   6,   1,   3,  16,   3,   3,\n",
       "           1,   5,  14,   2], dtype=torch.int32),\n",
       " 'amount_rur': tensor([2.0517e+01, 1.7402e+01, 8.6550e+00, 1.2141e+01, 2.2898e+01, 4.3130e+00,\n",
       "         5.3910e+01, 1.7494e+01, 2.2394e+01, 1.0407e+02, 2.4065e+01, 2.7020e+00,\n",
       "         2.4442e+01, 1.0191e+02, 2.6340e+00, 1.2365e+01, 2.7947e+01, 3.9201e+01,\n",
       "         1.7410e+00, 2.5050e+01, 1.6784e+01, 5.9530e+00, 4.7352e+01, 3.1955e+01,\n",
       "         8.3667e+01, 1.2205e+02, 5.6780e+00, 1.3738e+01, 3.7768e+01, 1.6388e+01,\n",
       "         4.1130e+00, 2.0150e+01, 1.4441e+02, 2.0058e+01, 4.6930e+00, 2.8900e+00,\n",
       "         8.2400e-01, 5.1750e+00, 4.3130e+00, 4.4422e+01, 1.8272e+01, 9.1860e+00,\n",
       "         1.5457e+02, 8.6250e+00, 3.1140e+00, 1.0258e+01, 6.8617e+01, 4.2260e+00,\n",
       "         1.0868e+01, 6.4691e+01, 8.9710e+00, 2.4317e+01, 3.4350e+00, 4.9170e+00,\n",
       "         3.6777e+01, 2.2414e+01, 2.0608e+01, 5.4910e+00, 3.4502e+01, 1.9332e+01,\n",
       "         8.3800e+00, 9.8770e+00, 4.2219e+01, 2.3280e+00, 1.6894e+01, 4.0367e+02,\n",
       "         7.9212e+01, 3.7133e+01, 3.6640e+00, 1.2410e+01, 8.8750e+00, 3.1175e+02,\n",
       "         3.0774e+01, 1.6029e+01, 3.1865e+01, 1.7632e+01, 5.9530e+00, 2.1560e+00,\n",
       "         1.9299e+01, 3.5710e+01, 1.3887e+01, 2.3238e+01, 9.2724e+01, 1.9278e+01,\n",
       "         1.1515e+01, 8.2430e+00, 1.0104e+02, 1.4060e+01, 2.0607e+01, 1.2895e+01,\n",
       "         2.6761e+01, 1.2895e+01, 2.5877e+01, 1.5500e+02, 4.5790e+00, 1.6906e+01,\n",
       "         6.5259e+01, 2.2258e+01, 1.1907e+01, 5.1788e+02, 1.3730e+00, 5.1310e+00,\n",
       "         2.3193e+02, 1.8230e+00, 2.2732e+02, 1.7190e+01, 7.7390e+00, 8.2370e+00,\n",
       "         2.0269e+01, 9.9840e+00, 4.3505e+01, 2.1801e+01, 7.3270e+00, 1.2525e+01,\n",
       "         1.0829e+02, 1.1861e+01, 1.5095e+02, 8.7440e+00, 1.1183e+02, 3.9356e+01,\n",
       "         6.0770e+01, 7.3270e+00, 6.4691e+01, 2.5180e+00, 5.6070e+00, 6.2740e+00,\n",
       "         4.9916e+01, 4.1220e+00, 5.4950e+00, 4.3084e+01, 1.2938e+01, 1.2639e+01,\n",
       "         4.3130e+00, 1.6029e+01, 2.5209e+02, 1.5662e+01, 2.4180e+01, 5.6066e+01,\n",
       "         1.8319e+01, 8.7722e+01, 2.0379e+01, 2.6195e+01, 2.2446e+01, 3.4256e+01,\n",
       "         4.1220e+00, 1.4670e+00, 9.7090e+00, 1.3671e+01, 5.3480e+00, 2.5577e+01,\n",
       "         2.8850e+00, 4.6577e+01, 1.0963e+02, 4.9640e+00, 7.9780e+00, 1.9408e+01,\n",
       "         1.9611e+01, 5.6490e+00, 2.7477e+01, 7.3594e+01, 9.1590e+00, 7.1660e+00,\n",
       "         1.5112e+01, 8.6736e+01, 2.7483e+01, 1.8250e+01, 4.6624e+02, 1.1448e+01,\n",
       "         2.6332e+01, 1.5579e+01, 1.9321e+01, 1.5725e+01, 1.0782e+01, 2.9320e+00,\n",
       "         4.5474e+01, 4.2610e+01, 5.1750e+00, 5.8068e+01, 1.8762e+02, 4.7010e+00,\n",
       "         3.1612e+01, 2.4546e+02, 2.2898e+01, 1.0592e+02, 1.0644e+02, 6.0450e+01,\n",
       "         3.1570e+01, 4.9000e+00, 2.5900e-01, 5.9530e+00, 1.0885e+01, 1.8272e+01,\n",
       "         2.7601e+01, 1.5490e+00, 9.1590e+00, 3.2428e+01, 5.0322e+01, 4.3130e+00,\n",
       "         4.1215e+01, 2.9544e+01, 3.3650e+01, 3.1141e+01, 2.1826e+02, 1.0954e+01,\n",
       "         8.0600e+01, 3.7090e+00, 2.7912e+01, 9.7150e+00, 1.5168e+01, 6.0300e-01,\n",
       "         3.3635e+01, 1.1357e+01, 6.3400e+00, 2.5600e+01, 1.4370e+02, 4.1674e+01,\n",
       "         3.4805e+01, 1.0932e+03, 8.8385e+01, 1.2649e+02, 2.0700e+00, 5.9950e+00,\n",
       "         2.6103e+01, 2.1504e+01, 2.4272e+01, 1.5470e+02, 1.1448e+01, 8.0233e+01,\n",
       "         1.3234e+01, 1.1515e+01, 2.2486e+01, 3.2057e+01, 2.8287e+01, 6.9010e+00,\n",
       "         4.2130e+00, 1.8243e+01, 5.3122e+01, 9.4800e+00, 1.3784e+01, 4.5498e+01,\n",
       "         1.3323e+01, 4.5790e+00, 3.3572e+02, 5.8939e+01, 5.4780e+00, 4.5790e+00,\n",
       "         4.5790e+00, 1.2360e+00, 7.2890e+00, 9.0570e+00, 1.2834e+01, 2.3353e+01,\n",
       "         1.3990e+02, 2.0610e+00, 6.9870e+00, 2.6510e+00, 9.1600e-01, 4.8023e+01,\n",
       "         3.6141e+01, 7.0858e+01, 8.6250e+00, 1.1610e+01, 3.9315e+01, 8.8525e+01,\n",
       "         8.0302e+01, 3.0190e+00, 2.9431e+01, 1.2145e+01, 1.0075e+01, 5.7250e+00,\n",
       "         2.3332e+02, 1.3693e+01, 8.3013e+01, 4.4500e+00, 1.0991e+01, 3.0620e+00,\n",
       "         7.0990e+00, 2.1607e+01, 5.3910e+00, 6.8693e+01, 8.7890e+00, 9.1590e+00,\n",
       "         3.2560e+01, 3.0707e+01, 2.5650e+00, 7.2173e+01, 2.7431e+01, 2.2256e+01,\n",
       "         3.6983e+01, 1.7680e+00, 4.0528e+01, 2.2900e+00, 1.1269e+03, 4.3130e+00,\n",
       "         9.3891e+01, 1.2938e+01, 2.4812e+01, 2.3289e+01, 1.6302e+01, 3.3980e+01,\n",
       "         1.4750e+01, 9.5987e+01, 3.0849e+01, 1.9321e+01, 1.7402e+01, 1.7402e+01,\n",
       "         7.1851e+01, 5.0964e+01, 2.4421e+01, 2.2770e+02, 4.2260e+00, 8.0828e+01,\n",
       "         6.0380e+00, 2.5421e+01, 8.3540e+00, 4.4413e+01, 6.2540e+00, 1.1083e+01,\n",
       "         2.0150e+00, 1.3738e+01, 3.2050e+00, 1.5871e+01, 7.6770e+00, 2.0110e+02,\n",
       "         4.1834e+01, 1.2292e+01, 4.3686e+01, 1.2937e+01, 4.4604e+01, 2.4042e+01,\n",
       "         1.9382e+01, 5.5139e+01, 2.7019e+01, 8.9200e+00, 9.0180e+01, 4.5280e+00,\n",
       "         2.2890e+01, 1.5896e+01, 1.2365e+01, 2.5445e+02, 6.2860e+00, 9.1590e+00,\n",
       "         7.1160e+00, 1.7251e+01, 6.0380e+00, 2.2426e+01, 1.0212e+01, 4.5795e+01,\n",
       "         3.3467e+01, 2.4776e+01, 2.9500e+00, 1.8318e+01, 9.3110e+00, 1.1220e+01,\n",
       "         2.9283e+01, 8.4230e+00, 6.7320e+00, 6.8830e+00, 7.2460e+00, 7.3180e+01,\n",
       "         2.0701e+01, 1.2206e+01, 9.8770e+00, 1.6666e+01, 4.3130e+00, 1.7173e+01,\n",
       "         8.8160e+00, 4.9367e+01, 7.4190e+00, 1.4271e+01, 5.4950e+00, 6.0380e+00,\n",
       "         9.0007e+01, 2.2436e+01, 8.6250e+00, 8.7930e+00, 3.4913e+01, 5.6070e+00,\n",
       "         4.3384e+01, 5.0934e+01, 3.0130e+00, 2.0150e+00, 4.4850e+00, 7.2810e+00,\n",
       "         4.5800e-01, 1.0533e+01, 6.7980e+00, 5.7271e+01, 5.5780e+00, 1.8319e+01,\n",
       "         2.1453e+01, 1.8970e+00, 3.0189e+01, 3.7471e+01, 9.9840e+01, 1.0135e+01,\n",
       "         2.2394e+01, 4.3130e+00, 1.0566e+01, 8.9750e+00, 3.5543e+01, 1.9830e+01,\n",
       "         4.5658e+01, 5.5870e+00, 3.5480e+01, 1.0534e+01, 2.5803e+01, 3.8010e+00,\n",
       "         6.6244e+01, 4.5795e+01, 2.9685e+01, 3.8506e+01, 4.5790e+00, 1.7247e+02,\n",
       "         6.6912e+01, 1.5870e+00, 9.5310e+00, 6.5490e+00, 6.8020e+00, 1.5112e+01,\n",
       "         1.3752e+02, 1.3198e+01, 2.6998e+01, 2.9715e+01, 1.0437e+01, 1.3171e+02,\n",
       "         1.5095e+01, 2.1633e+01, 2.5650e+00, 6.4120e+00, 2.2650e+02, 1.6934e+02,\n",
       "         1.4857e+02, 1.6715e+01, 2.2990e+01, 7.0070e+00, 5.3910e+01, 2.2898e+01,\n",
       "         2.7480e+00, 9.9840e+00, 8.6250e+00, 1.2730e+01, 2.2383e+01, 8.7930e+00,\n",
       "         2.4570e+00, 5.4780e+00, 1.3281e+02, 1.2135e+01, 2.6058e+01, 9.3400e-01,\n",
       "         8.1969e+01, 1.4362e+01, 3.5408e+01, 4.0300e+00, 2.5471e+01, 9.6350e+00,\n",
       "         3.4770e+00, 1.0221e+01, 6.6630e+00, 1.1127e+01, 2.6058e+01, 2.9930e+01,\n",
       "         4.3130e+00, 2.1361e+01, 6.2556e+01, 4.5542e+02, 2.0700e+00, 1.2507e+01,\n",
       "         9.6560e+00, 3.6170e+00, 4.8772e+01, 6.4068e+01, 4.5795e+02, 2.2908e+01,\n",
       "         2.2898e+01, 9.5127e+01, 1.7553e+01, 2.0917e+01, 1.1128e+01, 8.6255e+01,\n",
       "         1.1774e+01, 1.0606e+01, 7.2460e+00, 2.2073e+01, 1.4404e+01, 4.0109e+01,\n",
       "         9.1590e+00, 2.7825e+01, 3.8728e+01, 1.5526e+01, 5.5458e+01, 1.0898e+01,\n",
       "         2.0127e+01, 1.1403e+01, 2.3110e+01, 3.6170e+00, 3.1061e+01, 6.1810e+00,\n",
       "         4.3127e+01, 7.1031e+01, 1.9804e+01, 1.7251e+01, 6.8240e+00, 1.1601e+01,\n",
       "         1.8318e+02, 6.8693e+01, 2.7559e+01, 1.7906e+01, 1.8319e+01, 9.6990e+00,\n",
       "         6.8690e+00, 1.0629e+02, 2.5188e+01, 5.6155e+01, 7.1160e+00, 3.3579e+01,\n",
       "         1.2594e+01, 5.8948e+01, 3.8016e+01, 1.6302e+01, 5.7150e+00, 6.6350e+00,\n",
       "         8.1530e+00, 8.4396e+01, 4.9459e+02, 4.5154e+01, 7.2010e+00, 2.3573e+02,\n",
       "         4.3130e+00, 3.2332e+01, 1.9105e+01, 5.5592e+01, 1.2033e+01, 2.1564e+01,\n",
       "         2.5880e+00, 1.7104e+02, 1.4200e+00, 7.7630e+00, 5.0009e+01, 1.0006e+01,\n",
       "         8.9710e+00, 1.9816e+01, 2.2898e+01, 1.0373e+02, 7.1900e+00, 7.9665e+01,\n",
       "         1.3796e+01, 2.8153e+01, 4.5800e-01, 1.2501e+02, 5.9991e+01, 1.9234e+01,\n",
       "         6.8580e+00, 3.4500e+00, 6.8573e+02, 9.2208e+01, 7.2890e+00, 2.5833e+01,\n",
       "         1.7173e+01, 1.8319e+01, 2.6950e+01, 4.5633e+01, 3.1910e+00, 1.5003e+01,\n",
       "         1.6761e+01, 1.0524e+01, 5.2665e+01, 5.6007e+01, 1.4663e+01, 2.3810e+00,\n",
       "         4.4950e+00, 1.2938e+01, 2.0929e+01, 4.0681e+01, 1.1429e+01, 3.6377e+01,\n",
       "         1.8502e+01, 3.8770e+00, 3.4350e+00, 8.4870e+00, 6.7810e+00, 1.0006e+01,\n",
       "         7.5141e+01, 7.7630e+00, 9.8330e+00, 4.3127e+01, 7.9230e+00, 8.5830e+00,\n",
       "         9.1590e+00, 9.2990e+01, 1.3507e+01, 2.6277e+01, 6.6400e+00, 6.8240e+00,\n",
       "         6.6400e+00, 3.3208e+01, 9.8460e+01, 3.1598e+01, 2.2048e+01, 2.3172e+01,\n",
       "         1.6119e+01, 2.7480e+00, 5.5200e+00, 1.0991e+01, 3.3744e+01, 2.8363e+01,\n",
       "         2.6580e+00, 2.1560e+00, 5.6765e+01, 1.1156e+01, 1.5909e+01, 5.9947e+01,\n",
       "         1.3143e+01, 2.2898e+01, 4.5795e+01, 1.6600e+01, 1.6486e+01, 9.1130e+00,\n",
       "         1.3006e+01, 2.0658e+01, 4.3130e+00, 8.3119e+01, 7.1910e+00, 4.9726e+01,\n",
       "         3.4346e+01, 1.8868e+02, 3.2881e+01, 1.1731e+01, 7.5370e+01, 1.1847e+02,\n",
       "         2.1315e+01, 1.0350e+00, 6.8503e+01, 2.2091e+01, 1.3456e+01, 8.1950e+00,\n",
       "         9.1284e+01, 7.0560e+00, 3.3064e+01, 5.6786e+01, 2.8033e+01, 1.0782e+01,\n",
       "         2.0794e+01, 1.6707e+01, 6.0897e+01, 4.3127e+01, 1.2639e+01, 6.2910e+01,\n",
       "         2.2898e+01, 1.2096e+03, 4.4934e+01, 5.3849e+01, 3.6658e+01, 1.4472e+01,\n",
       "         6.5048e+01, 4.8303e+01, 3.1552e+02, 3.9768e+02, 3.3210e+00, 5.4125e+01,\n",
       "         1.6333e+02, 4.5790e+00, 6.1820e+00, 1.8272e+01, 6.0379e+01, 1.1907e+01,\n",
       "         8.1810e+00, 5.9359e+01, 2.7431e+01, 4.3130e+00, 8.7980e+00, 8.9962e+01,\n",
       "         6.6850e+00, 2.4995e+01, 4.3130e+00, 9.9370e+00, 1.9738e+01, 7.5562e+01,\n",
       "         6.5890e+00, 1.3281e+01, 1.8089e+01, 3.8513e+01, 5.1274e+01, 5.8179e+01,\n",
       "         6.1915e+01, 7.3270e+00, 3.8820e+00, 4.1215e+01, 7.3270e+00, 7.1160e+00,\n",
       "         5.9340e+00, 4.7820e+00, 1.2044e+01, 9.7681e+01, 9.4954e+01, 4.2351e+01,\n",
       "         7.4640e+00, 1.7082e+01, 5.4557e+01, 3.4310e+01, 1.5474e+02, 6.1061e+01,\n",
       "         2.4409e+01, 6.8235e+01, 1.1213e+01, 2.7947e+02, 1.9408e+01, 7.6340e+00,\n",
       "         1.8873e+01, 5.0700e+00, 1.0337e+01, 1.4416e+02, 5.4450e+00, 4.4879e+01,\n",
       "         3.3682e+01, 1.6216e+01, 9.0022e+01, 6.2103e+01, 1.0936e+01, 8.6255e+01,\n",
       "         1.1036e+01, 2.3720e+01, 1.4792e+01, 2.3668e+01, 4.5790e+00, 6.4690e+00,\n",
       "         3.6440e+00, 9.1590e+00, 2.1564e+01, 4.6375e+01, 3.8772e+01, 2.2898e+01,\n",
       "         2.6613e+01, 1.3143e+02, 3.1313e+01, 2.2898e+01, 3.9035e+01, 1.6879e+02,\n",
       "         1.7242e+02, 7.0300e+00, 6.4570e+00, 1.0825e+01, 6.8918e+01, 3.4585e+01,\n",
       "         1.4608e+01, 5.7656e+01, 6.7319e+01, 2.8594e+01, 3.7781e+01, 5.8407e+01,\n",
       "         8.1296e+01, 8.6965e+01, 1.0428e+01, 8.8886e+01, 1.9666e+01, 3.4287e+01,\n",
       "         5.3480e+00, 2.1661e+01, 1.2766e+01, 4.5000e-02, 4.1057e+01, 5.6329e+01,\n",
       "         2.3297e+01, 5.5916e+01, 1.5180e+01, 2.4065e+01, 1.0671e+01, 3.0497e+01,\n",
       "         1.4563e+01, 6.1100e+00, 1.6302e+01, 5.5721e+01, 1.4357e+02, 6.4690e+00,\n",
       "         1.1731e+01, 6.8137e+01, 6.4691e+01, 2.3413e+01, 1.4425e+01, 4.5790e+00,\n",
       "         4.3198e+01, 2.0960e+02, 3.4483e+01, 4.3510e+00, 2.6539e+01, 4.7630e+00,\n",
       "         1.3633e+02, 1.5964e+02, 2.1564e+01, 4.2700e+00, 2.6351e+01, 1.1678e+01,\n",
       "         1.6945e+01, 7.8125e+01, 7.6020e+00, 3.3596e+01, 1.5708e+01, 8.7538e+01,\n",
       "         1.3762e+02, 2.4127e+01, 4.1368e+01, 4.8046e+02, 3.4500e+00, 4.4410e+00,\n",
       "         3.7090e+00, 4.3130e+00, 1.5242e+01, 7.5301e+01, 9.0570e+00, 3.4416e+01,\n",
       "         3.4502e+01, 5.0459e+01, 9.9840e+00, 1.1448e+01, 3.9008e+01, 2.3767e+01,\n",
       "         2.9823e+01, 4.3130e+00, 9.0670e+00, 1.4867e+01, 1.3046e+01, 3.0546e+01,\n",
       "         2.2898e+01, 1.3509e+01, 8.6250e+00, 1.3801e+01, 5.7250e+00, 2.2642e+01,\n",
       "         2.3677e+01, 6.5487e+01]),\n",
       " 'event_time': tensor([0.0011, 0.0013, 0.0018, 0.0022, 0.0031, 0.0046, 0.0060, 0.0063, 0.0078,\n",
       "         0.0109, 0.0113, 0.0131, 0.0150, 0.0154, 0.0160, 0.0165, 0.0176, 0.0185,\n",
       "         0.0210, 0.0253, 0.0258, 0.0269, 0.0273, 0.0273, 0.0285, 0.0325, 0.0325,\n",
       "         0.0336, 0.0362, 0.0373, 0.0375, 0.0405, 0.0415, 0.0431, 0.0444, 0.0448,\n",
       "         0.0494, 0.0522, 0.0529, 0.0560, 0.0563, 0.0565, 0.0569, 0.0585, 0.0585,\n",
       "         0.0585, 0.0592, 0.0594, 0.0600, 0.0621, 0.0640, 0.0647, 0.0649, 0.0665,\n",
       "         0.0692, 0.0714, 0.0733, 0.0733, 0.0738, 0.0750, 0.0757, 0.0759, 0.0772,\n",
       "         0.0820, 0.0830, 0.0852, 0.0877, 0.0898, 0.0906, 0.0916, 0.0921, 0.0928,\n",
       "         0.0932, 0.0957, 0.0959, 0.0980, 0.0995, 0.1002, 0.1006, 0.1039, 0.1042,\n",
       "         0.1048, 0.1050, 0.1063, 0.1068, 0.1068, 0.1075, 0.1084, 0.1089, 0.1109,\n",
       "         0.1126, 0.1145, 0.1177, 0.1178, 0.1186, 0.1193, 0.1250, 0.1261, 0.1268,\n",
       "         0.1285, 0.1292, 0.1318, 0.1325, 0.1332, 0.1333, 0.1344, 0.1362, 0.1362,\n",
       "         0.1365, 0.1382, 0.1395, 0.1399, 0.1413, 0.1455, 0.1461, 0.1474, 0.1477,\n",
       "         0.1508, 0.1513, 0.1522, 0.1530, 0.1532, 0.1573, 0.1575, 0.1577, 0.1584,\n",
       "         0.1597, 0.1597, 0.1603, 0.1615, 0.1648, 0.1655, 0.1668, 0.1674, 0.1709,\n",
       "         0.1712, 0.1714, 0.1736, 0.1749, 0.1755, 0.1758, 0.1761, 0.1764, 0.1778,\n",
       "         0.1779, 0.1780, 0.1785, 0.1787, 0.1787, 0.1813, 0.1836, 0.1852, 0.1852,\n",
       "         0.1866, 0.1867, 0.1897, 0.1916, 0.1921, 0.1929, 0.1938, 0.1939, 0.1948,\n",
       "         0.1950, 0.1951, 0.1954, 0.1979, 0.1997, 0.2013, 0.2022, 0.2027, 0.2043,\n",
       "         0.2065, 0.2091, 0.2108, 0.2110, 0.2115, 0.2122, 0.2136, 0.2137, 0.2142,\n",
       "         0.2155, 0.2160, 0.2161, 0.2183, 0.2185, 0.2190, 0.2200, 0.2202, 0.2206,\n",
       "         0.2219, 0.2227, 0.2229, 0.2247, 0.2252, 0.2271, 0.2295, 0.2300, 0.2323,\n",
       "         0.2328, 0.2345, 0.2351, 0.2364, 0.2370, 0.2394, 0.2422, 0.2448, 0.2462,\n",
       "         0.2473, 0.2523, 0.2531, 0.2549, 0.2554, 0.2562, 0.2577, 0.2592, 0.2596,\n",
       "         0.2625, 0.2643, 0.2643, 0.2654, 0.2662, 0.2725, 0.2731, 0.2789, 0.2791,\n",
       "         0.2796, 0.2805, 0.2829, 0.2831, 0.2831, 0.2835, 0.2857, 0.2859, 0.2866,\n",
       "         0.2873, 0.2886, 0.2889, 0.2902, 0.2938, 0.2946, 0.2948, 0.2958, 0.2986,\n",
       "         0.2991, 0.3001, 0.3022, 0.3026, 0.3032, 0.3035, 0.3035, 0.3056, 0.3075,\n",
       "         0.3096, 0.3113, 0.3116, 0.3121, 0.3140, 0.3145, 0.3157, 0.3158, 0.3175,\n",
       "         0.3177, 0.3181, 0.3196, 0.3196, 0.3197, 0.3197, 0.3215, 0.3248, 0.3249,\n",
       "         0.3264, 0.3273, 0.3282, 0.3286, 0.3333, 0.3359, 0.3361, 0.3372, 0.3378,\n",
       "         0.3384, 0.3401, 0.3405, 0.3426, 0.3436, 0.3437, 0.3441, 0.3444, 0.3447,\n",
       "         0.3452, 0.3479, 0.3489, 0.3501, 0.3515, 0.3519, 0.3521, 0.3523, 0.3530,\n",
       "         0.3531, 0.3539, 0.3579, 0.3582, 0.3584, 0.3589, 0.3590, 0.3593, 0.3611,\n",
       "         0.3612, 0.3632, 0.3640, 0.3663, 0.3670, 0.3673, 0.3683, 0.3686, 0.3701,\n",
       "         0.3708, 0.3735, 0.3745, 0.3746, 0.3777, 0.3781, 0.3784, 0.3799, 0.3801,\n",
       "         0.3812, 0.3819, 0.3844, 0.3861, 0.3915, 0.3921, 0.3933, 0.3939, 0.3941,\n",
       "         0.3945, 0.3946, 0.3952, 0.3957, 0.3969, 0.3998, 0.4004, 0.4012, 0.4028,\n",
       "         0.4028, 0.4034, 0.4037, 0.4067, 0.4075, 0.4080, 0.4095, 0.4101, 0.4107,\n",
       "         0.4112, 0.4133, 0.4136, 0.4148, 0.4151, 0.4154, 0.4161, 0.4166, 0.4178,\n",
       "         0.4199, 0.4203, 0.4203, 0.4256, 0.4257, 0.4272, 0.4290, 0.4295, 0.4315,\n",
       "         0.4320, 0.4321, 0.4341, 0.4357, 0.4371, 0.4379, 0.4396, 0.4411, 0.4425,\n",
       "         0.4428, 0.4435, 0.4435, 0.4448, 0.4451, 0.4454, 0.4460, 0.4466, 0.4478,\n",
       "         0.4539, 0.4543, 0.4560, 0.4569, 0.4572, 0.4574, 0.4616, 0.4618, 0.4621,\n",
       "         0.4635, 0.4670, 0.4688, 0.4719, 0.4750, 0.4759, 0.4768, 0.4801, 0.4872,\n",
       "         0.4905, 0.4914, 0.4916, 0.4920, 0.4943, 0.4946, 0.4959, 0.4962, 0.4966,\n",
       "         0.4973, 0.4973, 0.4984, 0.4996, 0.5008, 0.5020, 0.5042, 0.5089, 0.5103,\n",
       "         0.5104, 0.5123, 0.5123, 0.5127, 0.5141, 0.5148, 0.5151, 0.5167, 0.5181,\n",
       "         0.5242, 0.5254, 0.5267, 0.5272, 0.5274, 0.5302, 0.5321, 0.5323, 0.5329,\n",
       "         0.5377, 0.5390, 0.5394, 0.5396, 0.5406, 0.5416, 0.5426, 0.5430, 0.5463,\n",
       "         0.5491, 0.5511, 0.5514, 0.5529, 0.5538, 0.5545, 0.5546, 0.5640, 0.5643,\n",
       "         0.5650, 0.5650, 0.5656, 0.5667, 0.5667, 0.5685, 0.5693, 0.5708, 0.5711,\n",
       "         0.5735, 0.5750, 0.5767, 0.5776, 0.5777, 0.5814, 0.5818, 0.5835, 0.5841,\n",
       "         0.5842, 0.5850, 0.5857, 0.5870, 0.5871, 0.5874, 0.5920, 0.5925, 0.5927,\n",
       "         0.5936, 0.5938, 0.5953, 0.5962, 0.5967, 0.5994, 0.6010, 0.6016, 0.6026,\n",
       "         0.6026, 0.6027, 0.6029, 0.6043, 0.6044, 0.6054, 0.6071, 0.6087, 0.6094,\n",
       "         0.6097, 0.6098, 0.6122, 0.6126, 0.6137, 0.6138, 0.6143, 0.6165, 0.6166,\n",
       "         0.6176, 0.6184, 0.6207, 0.6255, 0.6261, 0.6279, 0.6280, 0.6283, 0.6285,\n",
       "         0.6291, 0.6293, 0.6295, 0.6303, 0.6312, 0.6313, 0.6325, 0.6343, 0.6345,\n",
       "         0.6346, 0.6371, 0.6392, 0.6420, 0.6424, 0.6430, 0.6438, 0.6446, 0.6455,\n",
       "         0.6462, 0.6471, 0.6476, 0.6481, 0.6491, 0.6504, 0.6528, 0.6537, 0.6542,\n",
       "         0.6545, 0.6550, 0.6561, 0.6566, 0.6588, 0.6602, 0.6603, 0.6603, 0.6608,\n",
       "         0.6636, 0.6646, 0.6651, 0.6680, 0.6697, 0.6753, 0.6753, 0.6759, 0.6767,\n",
       "         0.6767, 0.6768, 0.6790, 0.6826, 0.6847, 0.6853, 0.6926, 0.6929, 0.6949,\n",
       "         0.6962, 0.6995, 0.7007, 0.7013, 0.7020, 0.7023, 0.7046, 0.7059, 0.7090,\n",
       "         0.7118, 0.7129, 0.7133, 0.7136, 0.7142, 0.7160, 0.7166, 0.7170, 0.7171,\n",
       "         0.7222, 0.7227, 0.7228, 0.7235, 0.7240, 0.7246, 0.7251, 0.7276, 0.7287,\n",
       "         0.7289, 0.7294, 0.7295, 0.7297, 0.7311, 0.7338, 0.7339, 0.7347, 0.7365,\n",
       "         0.7375, 0.7380, 0.7400, 0.7402, 0.7407, 0.7426, 0.7466, 0.7478, 0.7484,\n",
       "         0.7494, 0.7512, 0.7542, 0.7557, 0.7559, 0.7560, 0.7563, 0.7585, 0.7598,\n",
       "         0.7601, 0.7601, 0.7623, 0.7638, 0.7655, 0.7659, 0.7669, 0.7682, 0.7691,\n",
       "         0.7693, 0.7698, 0.7699, 0.7708, 0.7725, 0.7730, 0.7741, 0.7756, 0.7758,\n",
       "         0.7787, 0.7788, 0.7792, 0.7797, 0.7812, 0.7836, 0.7842, 0.7858, 0.7888,\n",
       "         0.7889, 0.7905, 0.7961, 0.7966, 0.7974, 0.7977, 0.7977, 0.7989, 0.7996,\n",
       "         0.8008, 0.8008, 0.8009, 0.8011, 0.8013, 0.8025, 0.8027, 0.8036, 0.8057,\n",
       "         0.8073, 0.8108, 0.8128, 0.8133, 0.8140, 0.8148, 0.8157, 0.8159, 0.8170,\n",
       "         0.8172, 0.8183, 0.8194, 0.8233, 0.8276, 0.8300, 0.8306, 0.8317, 0.8330,\n",
       "         0.8379, 0.8383, 0.8407, 0.8408, 0.8410, 0.8421, 0.8429, 0.8433, 0.8463,\n",
       "         0.8476, 0.8497, 0.8508, 0.8516, 0.8536, 0.8554, 0.8560, 0.8565, 0.8580,\n",
       "         0.8582, 0.8583, 0.8589, 0.8632, 0.8644, 0.8677, 0.8688, 0.8693, 0.8717,\n",
       "         0.8726, 0.8768, 0.8771, 0.8773, 0.8775, 0.8793, 0.8806, 0.8820, 0.8829,\n",
       "         0.8834, 0.8868, 0.8875, 0.8889, 0.8893, 0.8894, 0.8901, 0.8918, 0.8918,\n",
       "         0.8921, 0.8926, 0.8930, 0.8951, 0.8978, 0.8983, 0.8990, 0.8990, 0.8990,\n",
       "         0.8993, 0.9000, 0.9007, 0.9043, 0.9046, 0.9047, 0.9050, 0.9057, 0.9082,\n",
       "         0.9108, 0.9123, 0.9126, 0.9141, 0.9158, 0.9173, 0.9198, 0.9220, 0.9229,\n",
       "         0.9235, 0.9236, 0.9255, 0.9277, 0.9292, 0.9310, 0.9317, 0.9321, 0.9338,\n",
       "         0.9348, 0.9356, 0.9366, 0.9369, 0.9378, 0.9381, 0.9410, 0.9417, 0.9432,\n",
       "         0.9459, 0.9461, 0.9480, 0.9485, 0.9499, 0.9520, 0.9521, 0.9533, 0.9541,\n",
       "         0.9543, 0.9550, 0.9550, 0.9553, 0.9581, 0.9590, 0.9599, 0.9619, 0.9619,\n",
       "         0.9639, 0.9645, 0.9671, 0.9676, 0.9677, 0.9678, 0.9683, 0.9683, 0.9687,\n",
       "         0.9693, 0.9707, 0.9716, 0.9736, 0.9817, 0.9834, 0.9839, 0.9868, 0.9870,\n",
       "         0.9870, 0.9887, 0.9901, 0.9913, 0.9935, 0.9967, 0.9970, 0.9977, 0.9988,\n",
       "         0.9997, 1.0000], dtype=torch.float64),\n",
       " 'target': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterable_inference_dataset = ParquetDataset(\n",
    "    data_files=ParquetFiles('df_gen_features.parquet').data_files,\n",
    "    i_filters=[\n",
    "        ISeqLenLimit(max_seq_len=2000), \n",
    "    ],\n",
    ")\n",
    "next(iter(iterable_inference_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for rec in tqdm(iterable_inference_dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68346c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': array(['u_100173', 'u_100319', 'u_100641', 'u_101300', 'u_101680',\n",
       "        'u_102378', 'u_102623', 'u_102630', 'u_10270', 'u_10290',\n",
       "        'u_103062', 'u_10341', 'u_104021', 'u_104027', 'u_104059',\n",
       "        'u_104186', 'u_104524', 'u_104751', 'u_104941', 'u_1051',\n",
       "        'u_105541', 'u_105590', 'u_105777', 'u_105901', 'u_105915',\n",
       "        'u_106161', 'u_106447', 'u_106565', 'u_106811', 'u_106945',\n",
       "        'u_10712', 'u_107123', 'u_10726', 'u_10731', 'u_107549',\n",
       "        'u_108468', 'u_108475', 'u_108574', 'u_108639', 'u_108647',\n",
       "        'u_108733', 'u_10909', 'u_109139', 'u_109210', 'u_109243',\n",
       "        'u_109369', 'u_109416', 'u_109500', 'u_10952', 'u_10954',\n",
       "        'u_109607', 'u_109666', 'u_110252', 'u_110405', 'u_110440',\n",
       "        'u_110466', 'u_110965', 'u_111464', 'u_1115', 'u_111552',\n",
       "        'u_111669', 'u_111673', 'u_111698', 'u_111798', 'u_111897',\n",
       "        'u_1123', 'u_112312', 'u_112923', 'u_112992', 'u_113182',\n",
       "        'u_113279', 'u_113305', 'u_113327', 'u_113634', 'u_113755',\n",
       "        'u_113939', 'u_114037', 'u_114134', 'u_11415', 'u_114172',\n",
       "        'u_114225', 'u_114239', 'u_114461', 'u_114564', 'u_114728',\n",
       "        'u_114763', 'u_115049', 'u_115148', 'u_115177', 'u_115594',\n",
       "        'u_115615', 'u_115737', 'u_115742', 'u_11614', 'u_116231',\n",
       "        'u_116304', 'u_116306', 'u_116500', 'u_116588', 'u_116626',\n",
       "        'u_116665', 'u_116755', 'u_117111', 'u_117122', 'u_117166',\n",
       "        'u_117820', 'u_117862', 'u_118011', 'u_11803', 'u_118201',\n",
       "        'u_118647', 'u_118683', 'u_119049', 'u_11908', 'u_119277',\n",
       "        'u_119285', 'u_119612', 'u_119741', 'u_119769', 'u_119824',\n",
       "        'u_119991', 'u_120189', 'u_12028', 'u_12042', 'u_120530',\n",
       "        'u_120556', 'u_120697', 'u_12075', 'u_120800', 'u_12081',\n",
       "        'u_12099', 'u_121098', 'u_121271', 'u_121368', 'u_121825',\n",
       "        'u_121913', 'u_12197', 'u_122065', 'u_122282', 'u_122515',\n",
       "        'u_122536', 'u_122814', 'u_122830', 'u_122908', 'u_123090',\n",
       "        'u_123170', 'u_123173', 'u_123331', 'u_123553', 'u_123572',\n",
       "        'u_123692', 'u_124041', 'u_124061', 'u_124342', 'u_124675',\n",
       "        'u_124704', 'u_124867', 'u_124954', 'u_125086', 'u_125160',\n",
       "        'u_125490', 'u_125636', 'u_125659', 'u_125680', 'u_12569',\n",
       "        'u_125782', 'u_126492', 'u_126954', 'u_127443', 'u_127520',\n",
       "        'u_127572', 'u_127679', 'u_128118', 'u_128309', 'u_12836',\n",
       "        'u_128527', 'u_128582', 'u_128656', 'u_128861', 'u_128883',\n",
       "        'u_129088', 'u_12922', 'u_129544', 'u_129564', 'u_129602',\n",
       "        'u_12968', 'u_12972', 'u_129892', 'u_129973', 'u_130016',\n",
       "        'u_13014', 'u_130517', 'u_130669', 'u_130815', 'u_131242',\n",
       "        'u_131277', 'u_131292', 'u_131293', 'u_131368', 'u_131811',\n",
       "        'u_131908', 'u_131915', 'u_132064', 'u_132151', 'u_132630',\n",
       "        'u_132919', 'u_133724', 'u_134038', 'u_134042', 'u_134125',\n",
       "        'u_134205', 'u_134444', 'u_134503', 'u_134603', 'u_13527',\n",
       "        'u_135597', 'u_13566', 'u_135733', 'u_135943', 'u_136024',\n",
       "        'u_136084', 'u_136263', 'u_136508', 'u_136543', 'u_136635',\n",
       "        'u_136663', 'u_13670', 'u_136867', 'u_137060', 'u_137661',\n",
       "        'u_137682', 'u_137778', 'u_137840', 'u_138001', 'u_138051',\n",
       "        'u_138209', 'u_138498', 'u_138881', 'u_138948', 'u_139204',\n",
       "        'u_13944', 'u_139496', 'u_13981', 'u_139971', 'u_140066',\n",
       "        'u_140077', 'u_140198', 'u_140255', 'u_14031', 'u_140486',\n",
       "        'u_140594', 'u_140612', 'u_140686', 'u_140777', 'u_140865',\n",
       "        'u_140944', 'u_141016', 'u_141147', 'u_141165', 'u_141207',\n",
       "        'u_141471', 'u_141578', 'u_141633', 'u_141774', 'u_141775',\n",
       "        'u_141996', 'u_142098', 'u_142176', 'u_142313', 'u_142444',\n",
       "        'u_142670', 'u_14305', 'u_143141', 'u_143676', 'u_14371',\n",
       "        'u_144027', 'u_144615', 'u_145242', 'u_145266', 'u_145460',\n",
       "        'u_145779', 'u_145946', 'u_146032', 'u_146144', 'u_146163',\n",
       "        'u_146671', 'u_146719', 'u_146974', 'u_147149', 'u_147394',\n",
       "        'u_147646', 'u_147862', 'u_148182', 'u_148793', 'u_148832',\n",
       "        'u_148959', 'u_149074', 'u_149281', 'u_149340', 'u_149405',\n",
       "        'u_149607', 'u_149670', 'u_149807', 'u_150022', 'u_150023',\n",
       "        'u_150320', 'u_150477', 'u_150665', 'u_150855', 'u_151047',\n",
       "        'u_151051', 'u_151057', 'u_151198', 'u_151216', 'u_151331',\n",
       "        'u_1514', 'u_15152', 'u_151570', 'u_15215', 'u_152240', 'u_152329',\n",
       "        'u_152414', 'u_152548', 'u_152636', 'u_152651', 'u_152923',\n",
       "        'u_15303', 'u_153282', 'u_153413', 'u_153528', 'u_153544',\n",
       "        'u_153576', 'u_153582', 'u_153740', 'u_153765', 'u_153854',\n",
       "        'u_154287', 'u_15451', 'u_154511', 'u_154733', 'u_15500',\n",
       "        'u_155025', 'u_155071', 'u_155212', 'u_155736', 'u_155781',\n",
       "        'u_155926', 'u_155934', 'u_156369', 'u_156712', 'u_156788',\n",
       "        'u_156838', 'u_15692', 'u_157180', 'u_157569', 'u_15768',\n",
       "        'u_15770', 'u_158009', 'u_158080', 'u_158821', 'u_158982',\n",
       "        'u_159285', 'u_159646', 'u_159764', 'u_159827', 'u_160112',\n",
       "        'u_160174', 'u_160306', 'u_160460', 'u_161383', 'u_161409',\n",
       "        'u_161550', 'u_161838', 'u_161852', 'u_162327', 'u_162334',\n",
       "        'u_162466', 'u_163099', 'u_163261', 'u_163283', 'u_163368',\n",
       "        'u_163775', 'u_164321', 'u_164340', 'u_164930', 'u_164970',\n",
       "        'u_165442', 'u_165555', 'u_165590', 'u_165729', 'u_166288',\n",
       "        'u_166702', 'u_16681', 'u_166893', 'u_166927', 'u_16703',\n",
       "        'u_167486', 'u_167955', 'u_168529', 'u_168617', 'u_16886',\n",
       "        'u_168962', 'u_169002', 'u_169200', 'u_169427', 'u_169516',\n",
       "        'u_169558', 'u_170015', 'u_170097', 'u_170357', 'u_170399',\n",
       "        'u_170530', 'u_170642', 'u_170768', 'u_170909', 'u_171264',\n",
       "        'u_171404', 'u_171612', 'u_171854', 'u_17191', 'u_171951',\n",
       "        'u_172131', 'u_172170', 'u_172216', 'u_172443', 'u_172507',\n",
       "        'u_172663', 'u_173094', 'u_173250', 'u_173294', 'u_173299',\n",
       "        'u_173454', 'u_173514', 'u_173553', 'u_173586', 'u_173720',\n",
       "        'u_173951', 'u_174618', 'u_174893', 'u_174968', 'u_175006',\n",
       "        'u_17560', 'u_175655', 'u_176573', 'u_176972', 'u_177315',\n",
       "        'u_177414', 'u_177495', 'u_178078', 'u_178179', 'u_17853',\n",
       "        'u_178539', 'u_178610', 'u_178670', 'u_178731', 'u_178899',\n",
       "        'u_178964', 'u_179478', 'u_179563', 'u_179676', 'u_179691',\n",
       "        'u_179824', 'u_179841', 'u_180079', 'u_180487', 'u_181118',\n",
       "        'u_181240', 'u_181575', 'u_181723', 'u_182080', 'u_18225',\n",
       "        'u_182633', 'u_183385', 'u_183403', 'u_183410', 'u_183574',\n",
       "        'u_183691', 'u_183877', 'u_183941', 'u_184159', 'u_184183',\n",
       "        'u_184239', 'u_184550', 'u_18462', 'u_184647', 'u_184948',\n",
       "        'u_185131', 'u_185181', 'u_185208', 'u_185559', 'u_185566',\n",
       "        'u_185579', 'u_185607', 'u_186036', 'u_186203', 'u_186486',\n",
       "        'u_186581', 'u_187108', 'u_187139', 'u_187234', 'u_187712',\n",
       "        'u_188257', 'u_188388', 'u_188459', 'u_188480', 'u_188499',\n",
       "        'u_188718', 'u_188859', 'u_189126', 'u_18915', 'u_189165',\n",
       "        'u_189221', 'u_189821', 'u_19014', 'u_19019', 'u_190204',\n",
       "        'u_190577', 'u_190693', 'u_190759', 'u_190903', 'u_191043',\n",
       "        'u_191065', 'u_191386', 'u_191505', 'u_191563', 'u_191895',\n",
       "        'u_192038', 'u_19268', 'u_192856', 'u_193071', 'u_193097',\n",
       "        'u_193247', 'u_193650', 'u_193985', 'u_194199', 'u_194293',\n",
       "        'u_194436', 'u_194520', 'u_194608', 'u_195291', 'u_195771',\n",
       "        'u_195895', 'u_195913', 'u_196028', 'u_196135', 'u_196265',\n",
       "        'u_196583', 'u_196804', 'u_196822', 'u_196902', 'u_196976',\n",
       "        'u_197268', 'u_197412', 'u_197447', 'u_197805', 'u_197908',\n",
       "        'u_198082', 'u_198128', 'u_198277', 'u_198523', 'u_198853',\n",
       "        'u_199713', 'u_199778', 'u_200101', 'u_200414', 'u_200795',\n",
       "        'u_200873', 'u_201014', 'u_201040', 'u_201150', 'u_20153',\n",
       "        'u_201673', 'u_201724', 'u_201791', 'u_201915', 'u_201933',\n",
       "        'u_202339', 'u_203119', 'u_203174', 'u_203185', 'u_203515',\n",
       "        'u_203711', 'u_203767', 'u_204072', 'u_204334', 'u_204346',\n",
       "        'u_204458', 'u_205153', 'u_205217', 'u_205372', 'u_205534',\n",
       "        'u_205572', 'u_205632', 'u_205904', 'u_20593', 'u_205933',\n",
       "        'u_206081', 'u_20616', 'u_206375', 'u_206534', 'u_206571',\n",
       "        'u_207305', 'u_207506', 'u_207995', 'u_208207', 'u_208213',\n",
       "        'u_208442', 'u_208598', 'u_208663', 'u_208928', 'u_209101',\n",
       "        'u_209159', 'u_209202', 'u_20939', 'u_209462', 'u_209480',\n",
       "        'u_209489', 'u_209910', 'u_209957', 'u_210028', 'u_210429',\n",
       "        'u_2106', 'u_210888', 'u_210940', 'u_211445', 'u_211514',\n",
       "        'u_211998', 'u_212', 'u_212257', 'u_212335', 'u_212474',\n",
       "        'u_212669', 'u_212811', 'u_213011', 'u_213658', 'u_214028',\n",
       "        'u_214254', 'u_21448', 'u_214741', 'u_214777', 'u_214926',\n",
       "        'u_21500', 'u_215203', 'u_21524', 'u_215282', 'u_21536',\n",
       "        'u_215735', 'u_21575', 'u_215776', 'u_215959', 'u_216278',\n",
       "        'u_216405', 'u_216727', 'u_216739', 'u_216903', 'u_217148',\n",
       "        'u_217200', 'u_217218', 'u_21733', 'u_217520', 'u_217525',\n",
       "        'u_217799', 'u_217841', 'u_218030', 'u_21806', 'u_218077',\n",
       "        'u_218086', 'u_218204', 'u_218575', 'u_219026', 'u_219197',\n",
       "        'u_219433', 'u_219464', 'u_219606', 'u_219711', 'u_219749',\n",
       "        'u_219767', 'u_220108', 'u_220223', 'u_220265', 'u_220385',\n",
       "        'u_221117', 'u_221162', 'u_221661', 'u_221864', 'u_221870',\n",
       "        'u_221961', 'u_221971', 'u_222020', 'u_222036', 'u_222077',\n",
       "        'u_22210', 'u_222680', 'u_223276', 'u_2233', 'u_223883',\n",
       "        'u_223954', 'u_224086', 'u_224304', 'u_224690', 'u_224990',\n",
       "        'u_225220', 'u_225354', 'u_22538', 'u_226027', 'u_226028',\n",
       "        'u_226099', 'u_226126', 'u_226565', 'u_227290', 'u_22733',\n",
       "        'u_227413', 'u_227537', 'u_227606', 'u_228238', 'u_228432',\n",
       "        'u_228460', 'u_228537', 'u_228717', 'u_228921', 'u_228998',\n",
       "        'u_229225', 'u_229294', 'u_229327', 'u_229452', 'u_229474',\n",
       "        'u_229506', 'u_229561', 'u_230299', 'u_230873', 'u_230894',\n",
       "        'u_231137', 'u_231185', 'u_231291', 'u_231520', 'u_231563',\n",
       "        'u_232040', 'u_232118', 'u_232172', 'u_23229', 'u_232550',\n",
       "        'u_232745', 'u_233141', 'u_233492', 'u_233859', 'u_233958',\n",
       "        'u_233987', 'u_234207', 'u_234251', 'u_234356', 'u_234652',\n",
       "        'u_234890', 'u_236079', 'u_236259', 'u_236620', 'u_236945',\n",
       "        'u_237567', 'u_237660', 'u_237852', 'u_237956', 'u_238005',\n",
       "        'u_238087', 'u_238154', 'u_23832', 'u_238373', 'u_238512',\n",
       "        'u_238525', 'u_238529', 'u_238721', 'u_238927', 'u_238939',\n",
       "        'u_239287', 'u_240183', 'u_240303', 'u_24031', 'u_240422',\n",
       "        'u_241160', 'u_241713', 'u_24181', 'u_241815', 'u_241888',\n",
       "        'u_24199', 'u_242442', 'u_242545', 'u_242708', 'u_242736',\n",
       "        'u_242803', 'u_242830', 'u_242872', 'u_242904', 'u_243285',\n",
       "        'u_243334', 'u_243413', 'u_243431', 'u_243439', 'u_243717',\n",
       "        'u_243790', 'u_244122', 'u_244661', 'u_244913', 'u_245219',\n",
       "        'u_245292', 'u_24584', 'u_246165', 'u_246387', 'u_246514',\n",
       "        'u_246542', 'u_246570', 'u_246742', 'u_246818', 'u_246955',\n",
       "        'u_247133', 'u_247520', 'u_247589', 'u_247637', 'u_247762',\n",
       "        'u_247872', 'u_247908', 'u_247943', 'u_247987', 'u_248080',\n",
       "        'u_24847', 'u_248645', 'u_248711', 'u_248830', 'u_249218',\n",
       "        'u_249241', 'u_249331', 'u_249649', 'u_249669', 'u_250063',\n",
       "        'u_250480', 'u_250491', 'u_251028', 'u_251354', 'u_251447',\n",
       "        'u_25151', 'u_251689', 'u_251723', 'u_252090', 'u_252167',\n",
       "        'u_252390', 'u_252853', 'u_253147', 'u_253282', 'u_253715',\n",
       "        'u_253830', 'u_253893', 'u_254253', 'u_254583', 'u_254719',\n",
       "        'u_254968', 'u_255355', 'u_255671', 'u_255886', 'u_255942',\n",
       "        'u_256139', 'u_256837', 'u_257156', 'u_257674', 'u_258151',\n",
       "        'u_258439', 'u_258518', 'u_258587', 'u_258821', 'u_259119',\n",
       "        'u_259481', 'u_259503', 'u_259945', 'u_259955', 'u_260444',\n",
       "        'u_260916', 'u_260926', 'u_261454', 'u_261472', 'u_261526',\n",
       "        'u_261600', 'u_261843', 'u_262068', 'u_26226', 'u_262458',\n",
       "        'u_262551', 'u_26259', 'u_262647', 'u_262871', 'u_262895',\n",
       "        'u_262924', 'u_262947', 'u_263052', 'u_263152', 'u_263490',\n",
       "        'u_263634', 'u_264013', 'u_264083', 'u_264337', 'u_264362',\n",
       "        'u_264577', 'u_264663', 'u_264991', 'u_265145', 'u_265174',\n",
       "        'u_26527', 'u_265355', 'u_265738', 'u_26589', 'u_266135',\n",
       "        'u_266175', 'u_266319', 'u_266698', 'u_266880', 'u_266932',\n",
       "        'u_266934', 'u_267124', 'u_267272', 'u_26729', 'u_267304',\n",
       "        'u_267308', 'u_26752', 'u_267597', 'u_268067', 'u_268073',\n",
       "        'u_268402', 'u_268415', 'u_268545', 'u_26855', 'u_269081',\n",
       "        'u_269135', 'u_269143', 'u_269163', 'u_2692', 'u_269488',\n",
       "        'u_26962', 'u_270329', 'u_270418', 'u_27071', 'u_270725',\n",
       "        'u_27124', 'u_271283', 'u_271491', 'u_271838', 'u_272119',\n",
       "        'u_272286', 'u_272294', 'u_27236', 'u_272573', 'u_272836',\n",
       "        'u_273615', 'u_273617', 'u_273949', 'u_274255', 'u_274275',\n",
       "        'u_274312', 'u_275128', 'u_275226', 'u_275284', 'u_275491',\n",
       "        'u_275759', 'u_275839', 'u_275908', 'u_276061', 'u_276394',\n",
       "        'u_276570', 'u_276634', 'u_276812', 'u_277076', 'u_277180',\n",
       "        'u_277233', 'u_277482', 'u_278035', 'u_278056', 'u_278168',\n",
       "        'u_27826', 'u_27867', 'u_279236', 'u_279695', 'u_279727',\n",
       "        'u_279907', 'u_280076', 'u_280078', 'u_280207', 'u_280266',\n",
       "        'u_28033', 'u_280920', 'u_281198', 'u_281261', 'u_281676',\n",
       "        'u_281984', 'u_282304', 'u_282306', 'u_282361', 'u_282476',\n",
       "        'u_282491', 'u_282512', 'u_28253', 'u_283189', 'u_283239',\n",
       "        'u_283484', 'u_283575', 'u_28365', 'u_283983'], dtype='<U8'),\n",
       " 'trans_date': tensor([[471, 430, 185,  ...,   0,   0,   0],\n",
       "         [295, 140, 262,  ...,   0,   0,   0],\n",
       "         [288, 182, 471,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [302, 195, 251,  ...,   0,   0,   0],\n",
       "         [ 44, 338, 652,  ...,   0,   0,   0],\n",
       "         [524, 103, 345,  ...,   0,   0,   0]], dtype=torch.int32),\n",
       " 'small_group': tensor([[ 2,  6, 10,  ...,  0,  0,  0],\n",
       "         [ 1, 12, 25,  ...,  0,  0,  0],\n",
       "         [ 8,  1,  9,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 1,  9,  9,  ...,  0,  0,  0],\n",
       "         [ 1, 26,  1,  ...,  0,  0,  0],\n",
       "         [54,  2,  4,  ...,  0,  0,  0]], dtype=torch.int32),\n",
       " 'amount_rur': tensor([[ 20.5170,  17.4020,   8.6550,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [156.1360,   1.8180, 198.3870,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [  8.5180,  15.7250,   3.8820,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [ 15.0190,  32.0570,   4.3130,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [  1.8310,  54.2210,  68.6600,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [ 15.0950,  71.4690,  16.9900,  ...,   0.0000,   0.0000,   0.0000]]),\n",
       " 'event_time': tensor([[0.0011, 0.0013, 0.0018,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0010, 0.0014, 0.0014,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0011, 0.0030, 0.0049,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0007, 0.0021, 0.0036,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0005, 0.0008, 0.0025,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0050, 0.0052, 0.0055,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        dtype=torch.float64),\n",
       " 'target': tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "         0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "         0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "         0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dl = torch.utils.data.DataLoader(\n",
    "    dataset=iterable_inference_dataset,\n",
    "    collate_fn=partial(collate_feature_dict, only_tensors=False),\n",
    "    shuffle=False,\n",
    "    batch_size=1000,\n",
    "    num_workers=12,\n",
    ")\n",
    "next(iter(inference_dl)).payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dda97025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce4ac9effa746619ba46c44dd242f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 2.16 s, total: 3.81 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for rec in tqdm(inference_dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62263aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "819d70d1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85cfc731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.inference_module import InferenceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bde712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random model in demo\n",
    "# use trained model for real usage\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(\n",
    "        embeddings={\n",
    "            'trans_date': {'in': 800, 'out': 32}, \n",
    "            'small_group': {'in': 200, 'out': 32},\n",
    "        },\n",
    "        numeric_values={'amount_rur': 'log'},\n",
    "    ),\n",
    "    hidden_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18ccbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InferenceModule(model=seq_encoder, pandas_output=True, model_out_name='emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e6562",
   "metadata": {},
   "source": [
    "## Iterable inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c52b52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kireev/pipenv_envs/pytorch-lifestream-KxQJF1XF/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd5499247314f6492c8b42512ab9fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict = pl.Trainer(gpus=1).predict(model, inference_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "231ca698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>emb_0000</th>\n",
       "      <th>emb_0001</th>\n",
       "      <th>emb_0002</th>\n",
       "      <th>emb_0003</th>\n",
       "      <th>emb_0004</th>\n",
       "      <th>emb_0005</th>\n",
       "      <th>emb_0006</th>\n",
       "      <th>emb_0007</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_0246</th>\n",
       "      <th>emb_0247</th>\n",
       "      <th>emb_0248</th>\n",
       "      <th>emb_0249</th>\n",
       "      <th>emb_0250</th>\n",
       "      <th>emb_0251</th>\n",
       "      <th>emb_0252</th>\n",
       "      <th>emb_0253</th>\n",
       "      <th>emb_0254</th>\n",
       "      <th>emb_0255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u_100173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.328473</td>\n",
       "      <td>-0.041245</td>\n",
       "      <td>-0.147720</td>\n",
       "      <td>-0.002845</td>\n",
       "      <td>0.312168</td>\n",
       "      <td>0.037303</td>\n",
       "      <td>0.393641</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245151</td>\n",
       "      <td>0.066214</td>\n",
       "      <td>0.114227</td>\n",
       "      <td>0.225143</td>\n",
       "      <td>-0.084436</td>\n",
       "      <td>-0.180903</td>\n",
       "      <td>0.151792</td>\n",
       "      <td>-0.016717</td>\n",
       "      <td>-0.045071</td>\n",
       "      <td>-0.450113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u_100319</td>\n",
       "      <td>1</td>\n",
       "      <td>0.238145</td>\n",
       "      <td>0.193970</td>\n",
       "      <td>0.147273</td>\n",
       "      <td>-0.071919</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>0.035626</td>\n",
       "      <td>0.112520</td>\n",
       "      <td>0.161449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178949</td>\n",
       "      <td>-0.139139</td>\n",
       "      <td>-0.207757</td>\n",
       "      <td>0.096016</td>\n",
       "      <td>-0.208386</td>\n",
       "      <td>0.182478</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.219091</td>\n",
       "      <td>0.176602</td>\n",
       "      <td>-0.064075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u_100641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143543</td>\n",
       "      <td>0.121639</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>0.173832</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>0.066611</td>\n",
       "      <td>0.395496</td>\n",
       "      <td>0.052862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090137</td>\n",
       "      <td>0.099573</td>\n",
       "      <td>-0.015959</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>-0.159012</td>\n",
       "      <td>0.278187</td>\n",
       "      <td>-0.296812</td>\n",
       "      <td>0.023095</td>\n",
       "      <td>-0.229518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u_101300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343369</td>\n",
       "      <td>-0.009261</td>\n",
       "      <td>0.120017</td>\n",
       "      <td>-0.194460</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>-0.105037</td>\n",
       "      <td>0.307335</td>\n",
       "      <td>0.112195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210593</td>\n",
       "      <td>0.054962</td>\n",
       "      <td>-0.185053</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>-0.349481</td>\n",
       "      <td>0.320525</td>\n",
       "      <td>0.327023</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>0.308533</td>\n",
       "      <td>-0.242240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u_101680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110057</td>\n",
       "      <td>0.382863</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>0.061691</td>\n",
       "      <td>0.146916</td>\n",
       "      <td>0.199971</td>\n",
       "      <td>0.349496</td>\n",
       "      <td>0.323261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116795</td>\n",
       "      <td>0.223603</td>\n",
       "      <td>-0.077991</td>\n",
       "      <td>0.112117</td>\n",
       "      <td>-0.079900</td>\n",
       "      <td>-0.158205</td>\n",
       "      <td>0.314365</td>\n",
       "      <td>0.177920</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>-0.025890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>u_283239</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354072</td>\n",
       "      <td>0.092454</td>\n",
       "      <td>0.040970</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>0.082783</td>\n",
       "      <td>-0.028416</td>\n",
       "      <td>0.233225</td>\n",
       "      <td>0.067131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273239</td>\n",
       "      <td>0.087559</td>\n",
       "      <td>-0.060689</td>\n",
       "      <td>-0.005826</td>\n",
       "      <td>-0.274094</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>0.256658</td>\n",
       "      <td>-0.059108</td>\n",
       "      <td>0.258162</td>\n",
       "      <td>-0.224057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>u_283484</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.117449</td>\n",
       "      <td>-0.052256</td>\n",
       "      <td>0.248940</td>\n",
       "      <td>0.008882</td>\n",
       "      <td>-0.080366</td>\n",
       "      <td>0.177775</td>\n",
       "      <td>0.151045</td>\n",
       "      <td>0.249522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>0.155122</td>\n",
       "      <td>-0.190573</td>\n",
       "      <td>-0.045708</td>\n",
       "      <td>-0.195466</td>\n",
       "      <td>0.071780</td>\n",
       "      <td>0.132462</td>\n",
       "      <td>0.121086</td>\n",
       "      <td>0.095586</td>\n",
       "      <td>-0.275485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>u_283575</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072493</td>\n",
       "      <td>0.324913</td>\n",
       "      <td>-0.086386</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>0.317191</td>\n",
       "      <td>0.390053</td>\n",
       "      <td>0.052547</td>\n",
       "      <td>-0.166408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067687</td>\n",
       "      <td>0.117783</td>\n",
       "      <td>0.166526</td>\n",
       "      <td>0.138490</td>\n",
       "      <td>-0.157028</td>\n",
       "      <td>-0.099661</td>\n",
       "      <td>0.157664</td>\n",
       "      <td>-0.371687</td>\n",
       "      <td>0.308954</td>\n",
       "      <td>-0.428635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>u_28365</td>\n",
       "      <td>0</td>\n",
       "      <td>0.210521</td>\n",
       "      <td>0.146216</td>\n",
       "      <td>-0.280463</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>-0.006321</td>\n",
       "      <td>0.108624</td>\n",
       "      <td>0.317275</td>\n",
       "      <td>-0.093777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104618</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>0.055233</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>-0.164119</td>\n",
       "      <td>-0.006468</td>\n",
       "      <td>0.427857</td>\n",
       "      <td>0.034468</td>\n",
       "      <td>0.251725</td>\n",
       "      <td>-0.168443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>u_283983</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423284</td>\n",
       "      <td>-0.140055</td>\n",
       "      <td>0.057075</td>\n",
       "      <td>0.478234</td>\n",
       "      <td>0.105640</td>\n",
       "      <td>-0.288524</td>\n",
       "      <td>0.561705</td>\n",
       "      <td>-0.142499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100825</td>\n",
       "      <td>0.112692</td>\n",
       "      <td>-0.124378</td>\n",
       "      <td>0.107732</td>\n",
       "      <td>-0.247653</td>\n",
       "      <td>0.124084</td>\n",
       "      <td>0.285969</td>\n",
       "      <td>0.129925</td>\n",
       "      <td>0.092968</td>\n",
       "      <td>-0.169996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  target  emb_0000  emb_0001  emb_0002  emb_0003  emb_0004  \\\n",
       "0    u_100173       0  0.328473 -0.041245 -0.147720 -0.002845  0.312168   \n",
       "1    u_100319       1  0.238145  0.193970  0.147273 -0.071919  0.074686   \n",
       "2    u_100641       0  0.143543  0.121639  0.114885  0.173832  0.040646   \n",
       "3    u_101300       0  0.343369 -0.009261  0.120017 -0.194460  0.283304   \n",
       "4    u_101680       0  0.110057  0.382863 -0.000882  0.061691  0.146916   \n",
       "..        ...     ...       ...       ...       ...       ...       ...   \n",
       "995  u_283239       0  0.354072  0.092454  0.040970  0.359921  0.082783   \n",
       "996  u_283484       0 -0.117449 -0.052256  0.248940  0.008882 -0.080366   \n",
       "997  u_283575       1  0.072493  0.324913 -0.086386  0.028520  0.317191   \n",
       "998   u_28365       0  0.210521  0.146216 -0.280463  0.035839 -0.006321   \n",
       "999  u_283983       1  0.423284 -0.140055  0.057075  0.478234  0.105640   \n",
       "\n",
       "     emb_0005  emb_0006  emb_0007  ...  emb_0246  emb_0247  emb_0248  \\\n",
       "0    0.037303  0.393641  0.022853  ...  0.245151  0.066214  0.114227   \n",
       "1    0.035626  0.112520  0.161449  ...  0.178949 -0.139139 -0.207757   \n",
       "2    0.066611  0.395496  0.052862  ... -0.090137  0.099573 -0.015959   \n",
       "3   -0.105037  0.307335  0.112195  ...  0.210593  0.054962 -0.185053   \n",
       "4    0.199971  0.349496  0.323261  ...  0.116795  0.223603 -0.077991   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995 -0.028416  0.233225  0.067131  ...  0.273239  0.087559 -0.060689   \n",
       "996  0.177775  0.151045  0.249522  ...  0.131372  0.155122 -0.190573   \n",
       "997  0.390053  0.052547 -0.166408  ...  0.067687  0.117783  0.166526   \n",
       "998  0.108624  0.317275 -0.093777  ... -0.104618 -0.006103  0.055233   \n",
       "999 -0.288524  0.561705 -0.142499  ...  0.100825  0.112692 -0.124378   \n",
       "\n",
       "     emb_0249  emb_0250  emb_0251  emb_0252  emb_0253  emb_0254  emb_0255  \n",
       "0    0.225143 -0.084436 -0.180903  0.151792 -0.016717 -0.045071 -0.450113  \n",
       "1    0.096016 -0.208386  0.182478  0.149600  0.219091  0.176602 -0.064075  \n",
       "2    0.011106  0.024636 -0.159012  0.278187 -0.296812  0.023095 -0.229518  \n",
       "3    0.062762 -0.349481  0.320525  0.327023  0.033048  0.308533 -0.242240  \n",
       "4    0.112117 -0.079900 -0.158205  0.314365  0.177920 -0.022189 -0.025890  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995 -0.005826 -0.274094  0.036808  0.256658 -0.059108  0.258162 -0.224057  \n",
       "996 -0.045708 -0.195466  0.071780  0.132462  0.121086  0.095586 -0.275485  \n",
       "997  0.138490 -0.157028 -0.099661  0.157664 -0.371687  0.308954 -0.428635  \n",
       "998  0.254883 -0.164119 -0.006468  0.427857  0.034468  0.251725 -0.168443  \n",
       "999  0.107732 -0.247653  0.124084  0.285969  0.129925  0.092968 -0.169996  \n",
       "\n",
       "[1000 rows x 258 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e1457a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>emb_0000</th>\n",
       "      <th>emb_0001</th>\n",
       "      <th>emb_0002</th>\n",
       "      <th>emb_0003</th>\n",
       "      <th>emb_0004</th>\n",
       "      <th>emb_0005</th>\n",
       "      <th>emb_0006</th>\n",
       "      <th>emb_0007</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_0246</th>\n",
       "      <th>emb_0247</th>\n",
       "      <th>emb_0248</th>\n",
       "      <th>emb_0249</th>\n",
       "      <th>emb_0250</th>\n",
       "      <th>emb_0251</th>\n",
       "      <th>emb_0252</th>\n",
       "      <th>emb_0253</th>\n",
       "      <th>emb_0254</th>\n",
       "      <th>emb_0255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u_100173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.328473</td>\n",
       "      <td>-0.041245</td>\n",
       "      <td>-0.147720</td>\n",
       "      <td>-0.002845</td>\n",
       "      <td>0.312168</td>\n",
       "      <td>0.037303</td>\n",
       "      <td>0.393641</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245151</td>\n",
       "      <td>0.066214</td>\n",
       "      <td>0.114227</td>\n",
       "      <td>0.225143</td>\n",
       "      <td>-0.084436</td>\n",
       "      <td>-0.180903</td>\n",
       "      <td>0.151792</td>\n",
       "      <td>-0.016717</td>\n",
       "      <td>-0.045071</td>\n",
       "      <td>-0.450113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u_100319</td>\n",
       "      <td>1</td>\n",
       "      <td>0.238145</td>\n",
       "      <td>0.193970</td>\n",
       "      <td>0.147273</td>\n",
       "      <td>-0.071919</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>0.035626</td>\n",
       "      <td>0.112520</td>\n",
       "      <td>0.161449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178949</td>\n",
       "      <td>-0.139139</td>\n",
       "      <td>-0.207757</td>\n",
       "      <td>0.096016</td>\n",
       "      <td>-0.208386</td>\n",
       "      <td>0.182478</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.219091</td>\n",
       "      <td>0.176602</td>\n",
       "      <td>-0.064075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u_100641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143543</td>\n",
       "      <td>0.121639</td>\n",
       "      <td>0.114885</td>\n",
       "      <td>0.173832</td>\n",
       "      <td>0.040646</td>\n",
       "      <td>0.066611</td>\n",
       "      <td>0.395496</td>\n",
       "      <td>0.052862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090137</td>\n",
       "      <td>0.099573</td>\n",
       "      <td>-0.015959</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>-0.159012</td>\n",
       "      <td>0.278187</td>\n",
       "      <td>-0.296812</td>\n",
       "      <td>0.023095</td>\n",
       "      <td>-0.229518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u_101300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.343369</td>\n",
       "      <td>-0.009261</td>\n",
       "      <td>0.120017</td>\n",
       "      <td>-0.194460</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>-0.105037</td>\n",
       "      <td>0.307335</td>\n",
       "      <td>0.112195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210593</td>\n",
       "      <td>0.054962</td>\n",
       "      <td>-0.185053</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>-0.349481</td>\n",
       "      <td>0.320525</td>\n",
       "      <td>0.327023</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>0.308533</td>\n",
       "      <td>-0.242240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u_101680</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110057</td>\n",
       "      <td>0.382863</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>0.061691</td>\n",
       "      <td>0.146916</td>\n",
       "      <td>0.199971</td>\n",
       "      <td>0.349496</td>\n",
       "      <td>0.323261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116795</td>\n",
       "      <td>0.223603</td>\n",
       "      <td>-0.077991</td>\n",
       "      <td>0.112117</td>\n",
       "      <td>-0.079900</td>\n",
       "      <td>-0.158205</td>\n",
       "      <td>0.314365</td>\n",
       "      <td>0.177920</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>-0.025890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  target  emb_0000  emb_0001  emb_0002  emb_0003  emb_0004  \\\n",
       "0  u_100173       0  0.328473 -0.041245 -0.147720 -0.002845  0.312168   \n",
       "1  u_100319       1  0.238145  0.193970  0.147273 -0.071919  0.074686   \n",
       "2  u_100641       0  0.143543  0.121639  0.114885  0.173832  0.040646   \n",
       "3  u_101300       0  0.343369 -0.009261  0.120017 -0.194460  0.283304   \n",
       "4  u_101680       0  0.110057  0.382863 -0.000882  0.061691  0.146916   \n",
       "\n",
       "   emb_0005  emb_0006  emb_0007  ...  emb_0246  emb_0247  emb_0248  emb_0249  \\\n",
       "0  0.037303  0.393641  0.022853  ...  0.245151  0.066214  0.114227  0.225143   \n",
       "1  0.035626  0.112520  0.161449  ...  0.178949 -0.139139 -0.207757  0.096016   \n",
       "2  0.066611  0.395496  0.052862  ... -0.090137  0.099573 -0.015959  0.011106   \n",
       "3 -0.105037  0.307335  0.112195  ...  0.210593  0.054962 -0.185053  0.062762   \n",
       "4  0.199971  0.349496  0.323261  ...  0.116795  0.223603 -0.077991  0.112117   \n",
       "\n",
       "   emb_0250  emb_0251  emb_0252  emb_0253  emb_0254  emb_0255  \n",
       "0 -0.084436 -0.180903  0.151792 -0.016717 -0.045071 -0.450113  \n",
       "1 -0.208386  0.182478  0.149600  0.219091  0.176602 -0.064075  \n",
       "2  0.024636 -0.159012  0.278187 -0.296812  0.023095 -0.229518  \n",
       "3 -0.349481  0.320525  0.327023  0.033048  0.308533 -0.242240  \n",
       "4 -0.079900 -0.158205  0.314365  0.177920 -0.022189 -0.025890  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_predict = pd.concat(predict, axis=0)\n",
    "full_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc5dd87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 258)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35e0eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predict.to_parquet('full_predict.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57979aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2G\tfull_predict.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh full_predict.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f878e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptls",
   "language": "python",
   "name": "ptls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
